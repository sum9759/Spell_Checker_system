{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
    "\n",
    "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
    "\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes.\n",
    "\n",
    "Then split your data into a test set of 100 lines and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the necessary packages\n",
    "import nltk\n",
    "import re \n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "# Loading the file and reading it using readlines()\n",
    "filename = 'holbrook.txt'\n",
    "f = open(filename)\n",
    "textfile = f.readlines()\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Calculating length of file\n",
    "with open(filename) as f:\n",
    "    length = (sum(1 for _ in f))\n",
    "    \n",
    "# Work tokenizing the sentences in file.     \n",
    "tokens = []\n",
    "for i in range(0,length):\n",
    "    sentences = nltk.sent_tokenize(textfile[i])  \n",
    "    tokens.append([nltk.word_tokenize(textfile[i]) for sent in sentences]) # to word tokenize each sentence in file\n",
    "#print(tokens) \n",
    "\n",
    "\n",
    "# Transforming list of list of list to a list of list.\n",
    "list_tokens = []\n",
    "for j in range(0, len(tokens)):\n",
    "    list_tokens.append(tokens[j][0])\n",
    "\n",
    "\n",
    "# Importing defaultdict library from collections\n",
    "from collections import defaultdict\n",
    "\n",
    "original = defaultdict(list)\n",
    "corrected = defaultdict(list)\n",
    "index = defaultdict(list)\n",
    "\n",
    "# For the length of the list_tokens created before, for each of the sentences, I am extracting the word to the left of the \n",
    "# separator(|) in original dict and right as corrected. Finally, the index of word containing | is saved as index and appended\n",
    "for i in range(0, (len(list_tokens))):\n",
    "    for j in range(0,(len(list_tokens[i]) )):\n",
    "        if(list_tokens[i][j].find('|') > 0):\n",
    "            split_token = list_tokens[i][j].split(\"|\")\n",
    "            original[i].append(split_token[0])\n",
    "            corrected[i].append(split_token[1])\n",
    "            index[i].append(j)\n",
    "        else:\n",
    "            original[i].append(list_tokens[i][j])\n",
    "            corrected[i].append(list_tokens[i][j])\n",
    "             \n",
    "# All the 3 dict lists of original sentences, corrected sentences and indexes are put in one dict called data\n",
    "data = defaultdict(list)\n",
    "for i in range(0,len(original)):\n",
    "    data[i].append(original[i])\n",
    "    data[i].append(corrected[i])\n",
    "    data[i].append(index[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['My', 'siter', 'go', 'to', 'Tonbury', '.'],\n",
       " ['My', 'sister', 'goes', 'to', 'Tonbury', '.'],\n",
       " [1, 2]]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking contents from data[6]\n",
    "data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and test datasets from data dict created before\n",
    "# As the first two lines of data have no interesting text except page and book title, I have put first 102 sentences \n",
    "# into test and rest in training\n",
    "\n",
    "test_data = defaultdict(list)\n",
    "for i in range(0,len(data)-1115):\n",
    "    test_data[i]=data[i]\n",
    "\n",
    "    \n",
    "training_data = defaultdict(list)  \n",
    "j = 0 # j is used to control indices and start from 0 instead of 102 in case of i\n",
    "for i in range(102,len(data)):\n",
    "    training_data[j]=data[i]\n",
    "    j = j+1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['My', 'Dad', 'works', 'at', 'Melton', '.'],\n",
       " ['My', 'Dad', 'works', 'at', 'Melton', '.'],\n",
       " []]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at contents of test_data generated\n",
    "test_data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2**: \n",
    "Calculate the frequency (number of occurrences), *ignoring case*, of all words and bigrams (sequences of two words) from the corrected *training* sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing ngrams library from nltk to form bigrams and trigrams later\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Empty list\n",
    "data_train = []\n",
    "\n",
    "# The correct sentences are stored in 1st index of training_data dictionary. Getting the data into a list\n",
    "for i in range(0,len(training_data)):\n",
    "    data_train.append(training_data[i][1])\n",
    "\n",
    "# Converting the data_train to lower_case so as to ignore different cases of same word\n",
    "data_train = list(chain(*data_train)) # Creates one list of tokens.\n",
    "tokens = [x.lower() for x in data_train]\n",
    "\n",
    "# Function to calculate frequency of unigrams\n",
    "def unigram(word):\n",
    "    count = 0\n",
    "    for u in tokens:\n",
    "        if(word == u):\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# Function to calculate frequency of bigrams. \n",
    "def bigram(words):\n",
    "    count = 0    \n",
    "    bigrams = ngrams(tokens,2) # Forming ngrams of size 2\n",
    "#     print (bigrams)\n",
    "    for b in bigrams:\n",
    "#         print (b)\n",
    "        b = b[0] + \" \" + b[1] # concatenating 2 elements of each bigram into a string to compare with train\n",
    "        if(words == b):\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "bigram(\"i like\")\n",
    "\n",
    "# Test your code with the following\n",
    "assert(unigram(\"me\")==87)\n",
    "assert(bigram(\"my mother\")==17)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 3**: \n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['heeloooo', 'hy', 'hii', 'hello']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance(\"hello\", \"hi\"))\n",
    "\n",
    "# Checking how edit distance works by taking different samples\n",
    "train_tokens = [\"hello\",\"heeloooo\",\"hii\",'hy','hii','hy','hy']\n",
    "list(set(train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
    "\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
    "\n",
    "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing edit distance library in nltk\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "        \n",
    "# Craeting a list of unique tokens from training data. This i did so that same candidates are not fetched more than once\n",
    "unique_tokens = []\n",
    "for i in range(0,len(training_data)):\n",
    "    unique_tokens.append([x.lower() for x in training_data[i][1]])\n",
    "\n",
    "# transforming list of list to a list\n",
    "unique_tokens = list(set(chain(*unique_tokens)))\n",
    "\n",
    "\n",
    "# Defining a function which fetches candidates based on their minimum distance from the input word\n",
    "def get_candidates(token):\n",
    "    result = []\n",
    "    min_edit_dist = 100 # initializing edit distance to a high number at first\n",
    "    \n",
    "    # For all words in set of unique_tokens, i check edit distance with input token\n",
    "    for word in unique_tokens:\n",
    "        current_dist = edit_distance(token, word)\n",
    "\n",
    "        # changing the minimum edit distance found\n",
    "        if(current_dist < min_edit_dist):\n",
    "            min_edit_dist = current_dist\n",
    "            \n",
    "    # appending all candidates to a list and returning that list\n",
    "    for word in unique_tokens:\n",
    "        if(edit_distance(token, word) == min_edit_dist):\n",
    "            result.append(word)\n",
    "    return (result)\n",
    "        \n",
    "# Test your code as follows\n",
    "assert(get_candidates(\"minde\") == ['mine', 'mind'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "\n",
    "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *bigram probability*. That is the candidate should be selected using the previous and following word in a bigram language model. Thus, if the $i$th word in a sentence is misspelled we should use the following to rank candidates:\n",
    "\n",
    "$$p(w_{i+1}|w_i) p(w_i|w_{i-1})$$\n",
    "\n",
    "For the first and last word of the sentence use only the conditional probabilities that exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the reqd libraries\n",
    "import nltk\n",
    "from itertools import chain\n",
    "\n",
    "# make a dictionary of all correct words from holbrook\n",
    "correct_tokens = []\n",
    "for i in range(len(data)):\n",
    "    correct_tokens.append([x.lower() for x in data[i][1]])\n",
    "\n",
    "# defining a set of correct tokens\n",
    "correct_tokens = list(chain.from_iterable(correct_tokens))\n",
    "\n",
    "    \n",
    "# Creating a function that will return corrected sentence for an incorrect one by comparing candidates of minimum edit \n",
    "# distance via maximum bigram probability, i.e. the chances of seeing the candidates given the previous word multiplied\n",
    "# by the chances of seeing the following word given the candidate word occurs. To prevent zero probabilities, I've added one\n",
    "# to probability calculation in numerator and denominator\n",
    "def correct(sentence):\n",
    "    \n",
    "    # defining an empty list to store the corrected sentence\n",
    "    correct = []\n",
    "    \n",
    "    # If the word encountered in sentence is in correct tokens, append it to correct list\n",
    "    for i in range(len(sentence)):\n",
    "        if(sentence[i] in correct_tokens):\n",
    "            correct.append(sentence[i])\n",
    "        else:\n",
    "            candidates = list(set(get_candidates(sentence[i]))) # get candidates for the word\n",
    "            \n",
    "            # unless the word has index 0 or is the last word, we calculate bigram probability using the normal formulae\n",
    "            if(sentence.index(sentence[i]) != 0 and sentence.index(sentence[i]) < (len(sentence) - 1)):\n",
    "                prec_word = sentence[i-1]\n",
    "                foll_word = sentence[i+1]\n",
    "                correct.append(bigram_prob(candidates, prec_word, foll_word ))\n",
    "            else:\n",
    "                index = sentence.index(sentence[i])\n",
    "                if(index == 0):\n",
    "                    \n",
    "                    # If the word has index 0, then we can only calculate its probability with the following word\n",
    "                    foll_word = sentence[i+1]\n",
    "                    correct.append(fl_bigram_prob(candidates,foll_word,index))\n",
    "                else:\n",
    "                    # For last word, we can only calculate probabaility with preceeding word\n",
    "                    prec_word = sentence[i-1]\n",
    "                    correct.append(fl_bigram_prob(candidates,prec_word,index))\n",
    "    return correct\n",
    "\n",
    "# Defining a function to calculate bigram_probability of seeing 2 words based on formulae given\n",
    "def bigram_prob(candidates,prec, foll ):\n",
    "    cond_prob = []\n",
    "    # For all the candidates, we calculate the bigram probabilities and store in list cond_prob\n",
    "    for j in range(0,len(candidates)):\n",
    "        cond_prob.append((calc_freq(prec,candidates[j]) +1)/(correct_tokens.count(prec) +1)*(calc_freq(candidates[j],foll) +1)/(correct_tokens.count(candidates[j]) +1))\n",
    "    \n",
    "    # out of all probabilities for the candidates, the max probability is chosen and that candidate is appended to correct\n",
    "    max_prob_index = cond_prob.index(max(cond_prob))\n",
    "    return candidates[max_prob_index]\n",
    "    \n",
    "# function to calculate bigram probabilities but only for 0 and last indices of sentences\n",
    "def fl_bigram_prob(candidates, word,index):\n",
    "    cond_prob = []\n",
    "    for j in range(0,len(candidates)):\n",
    "        if(index == 0):\n",
    "            cond_prob.append((calc_freq(candidates[j],word) + 1)/(correct_tokens.count(candidates[j]) +1 ))\n",
    "        else:\n",
    "            cond_prob.append((calc_freq(word,candidates[j]) + 1)/(correct_tokens.count(word) + 1))\n",
    "    max_prob_index = cond_prob.index(max(cond_prob))\n",
    "    return candidates[max_prob_index]\n",
    "\n",
    "# function that calculates the frequency of seeing a particular bigram\n",
    "def calc_freq(prec_word, foll_word):\n",
    "    count = 0\n",
    "    bigrams = nltk.bigrams(correct_tokens)\n",
    "    for b in bigrams:\n",
    "        if (b[0] == prec_word and b[1]== foll_word):\n",
    "            count = count + 1\n",
    "    return count\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 5**: \n",
    "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.81897302001741\n"
     ]
    }
   ],
   "source": [
    "# # subsetting test data into correct and incorrect sentences and checking accuracy\n",
    "data_test_incorrect = []\n",
    "data_test_correct = []\n",
    "index_test = []\n",
    "\n",
    "# Creating lower case lists of incorrect and correct sentences\n",
    "for i in range(0,len(test_data)):\n",
    "    data_test_incorrect.append([x.lower() for x in test_data[i][0]])\n",
    "    \n",
    "# test_data[i][0]\n",
    "for i in range(0,len(test_data)):\n",
    "    data_test_correct.append([x.lower() for x in test_data[i][1]])\n",
    "\n",
    "# storing the index of incorrect words for use in task 6\n",
    "for i in range(0,len(test_data)):\n",
    "    index_test.append(test_data[i][2])\n",
    "    \n",
    "# list to store predictions of correcting incorrect sentences\n",
    "predictions = []\n",
    "\n",
    "def accuracy(test):\n",
    "    count = 0\n",
    "    \n",
    "    # For the length of incorrect sentences list, we keep calling function correct on each sentence and storing \n",
    "    #the predictions\n",
    "    for j in range(len(data_test_incorrect)):\n",
    "        predictions.append(correct(data_test_incorrect[j]))\n",
    "    \n",
    "    # Breaking the sentences into words to be matched\n",
    "    p = list(chain.from_iterable(predictions))\n",
    "    d = list(chain.from_iterable(data_test_correct))     \n",
    "    \n",
    "    # Checking word match\n",
    "    for i in range(len(p)):  \n",
    "#         print(predictions[i])\n",
    "#         print(data_test_correct[i])\n",
    "        if (p[i] == d[i]):\n",
    "            count = count +1\n",
    "    \n",
    "    return (count/len(p) * 100) # matches found compared to length of entire prediction set\n",
    "  \n",
    "\n",
    "\n",
    "print(accuracy(data_test_incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '.']\n",
      "['1', '.']\n",
      "['1', '.']\n",
      "['nigel', 'thrush', 'page', '48']\n",
      "['nigel', 'thrush', 'page', '48']\n",
      "['nigel', 'thrush', 'page', '48']\n",
      "['i', 'have', 'four', 'in', 'my', 'family', 'dad', 'mum', 'and', 'siter', '.']\n",
      "['i', 'have', 'four', 'in', 'my', 'family', 'dad', 'mum', 'and', 'sister', '.']\n",
      "['i', 'have', 'four', 'in', 'my', 'family', 'dad', 'mum', 'and', 'sister', '.']\n",
      "['my', 'dad', 'works', 'at', 'melton', '.']\n",
      "['my', 'dad', 'works', 'at', 'melton', '.']\n",
      "['my', 'dad', 'works', 'at', 'melton', '.']\n",
      "['my', 'siter', 'go', 'to', 'tonbury', '.']\n",
      "['my', 'sister', 'go', 'to', 'tonbury', '.']\n",
      "['my', 'sister', 'goes', 'to', 'tonbury', '.']\n",
      "['my', 'mum', 'goes', 'out', 'some_times', '.']\n",
      "['my', 'mum', 'goes', 'out', 'sometimes', '.']\n",
      "['my', 'mum', 'goes', 'out', 'sometimes', '.']\n",
      "['i', 'go', 'to', 'bridgebrook', 'i', 'go', 'out', 'some_times', 'on', 'tuesday', 'night', 'i', 'go', 'to', 'youth', 'clob', '.']\n",
      "['i', 'go', 'to', 'bridgebrook', 'i', 'go', 'out', 'sometimes', 'on', 'tuesday', 'night', 'i', 'go', 'to', 'youth', 'club', '.']\n",
      "['i', 'go', 'to', 'bridgebrook', 'i', 'go', 'out', 'sometimes', 'on', 'tuesday', 'night', 'i', 'go', 'to', 'youth', 'club', '.']\n",
      "['on', 'thursday', 'nights', 'i', 'go', 'bell_ringing', 'on', 'saturdays', 'i', 'go', 'down', 'to', 'the', 'farm', '.']\n",
      "['on', 'thursday', 'nights', 'i', 'go', 'beginning', 'on', 'saturdays', 'i', 'go', 'down', 'to', 'the', 'farm', '.']\n",
      "['on', 'thursday', 'nights', 'i', 'go', 'bellringing', 'on', 'saturdays', 'i', 'go', 'down', 'to', 'the', 'farm', '.']\n",
      "['on', 'sundays', 'i', 'go', 'to', 'church', '.']\n",
      "['on', 'sundays', 'i', 'go', 'to', 'church', '.']\n",
      "['on', 'sundays', 'i', 'go', 'to', 'church', '.']\n",
      "['i', 'go', 'to', 'bed', 'at', '10', 'o', 'clock', 'i', 'wakh', 'tv', 'at', '5', 'o', 'clock', 'i', 'live', 'in', 'a', 'house', '.']\n",
      "['i', 'go', 'to', 'bed', 'at', '10', 'o', 'clock', 'i', 'wash', 'tv', 'at', '5', 'o', 'clock', 'i', 'live', 'in', 'a', 'house', '.']\n",
      "['i', 'go', 'to', 'bed', 'at', '10', 'o', 'clock', 'i', 'watch', 'tv', 'at', '5', 'o', 'clock', 'i', 'live', 'in', 'a', 'house', '.']\n",
      "['the', 'house', 'is', 'in', 'the', 'world', '.']\n",
      "['the', 'house', 'is', 'in', 'the', 'world', '.']\n",
      "['the', 'house', 'is', 'in', 'the', 'world', '.']\n",
      "['i', 'live', 'at', 'boar', 'parva', 'it', 'is', 'near', 'melton', 'and', 'bridgebrook', 'and', 'smallerden', '.']\n",
      "['i', 'live', 'at', 'boar', 'parva', 'it', 'is', 'near', 'melton', 'and', 'bridgebrook', 'and', 'smallerden', '.']\n",
      "['i', 'live', 'at', 'boar', 'parva', 'it', 'is', 'near', 'melton', 'and', 'bridgebrook', 'and', 'smallerden', '.']\n",
      "['the', 'house', 'is', 'white', 'it', 'has', 'stone', 'up', 'the', 'frount', 'it', 'is', 'the', 'first', 'from', 'bridgebrook', 'and', 'the', 'sexeon', 'from', 'smallerden', '.']\n",
      "['the', 'house', 'is', 'white', 'it', 'has', 'stone', 'up', 'the', 'front', 'it', 'is', 'the', 'first', 'from', 'bridgebrook', 'and', 'the', 'sexton', 'from', 'smallerden', '.']\n",
      "['the', 'house', 'is', 'white', 'it', 'has', 'stone', 'up', 'the', 'front', 'it', 'is', 'the', 'first', 'from', 'bridgebrook', 'and', 'the', 'second', 'from', 'smallerden', '.']\n",
      "['my', 'mum', 'is', 'at', 'home', 'she', 'goes', 'to', 'the', 'shop', 'on', 'fridays', 'my', 'dad', 'goes', 'to', 'work', 'at', 'smallerden', '.']\n",
      "['my', 'mum', 'is', 'at', 'home', 'she', 'goes', 'to', 'the', 'shop', 'on', 'fridays', 'my', 'dad', 'goes', 'to', 'work', 'at', 'smallerden', '.']\n",
      "['my', 'mum', 'is', 'at', 'home', 'she', 'goes', 'to', 'the', 'shop', 'on', 'fridays', 'my', 'dad', 'goes', 'to', 'work', 'at', 'smallerden', '.']\n",
      "['what', 'i', 'do', 'when', 'i', 'get', 'home', 'from', 'school', '.']\n",
      "['what', 'i', 'do', 'when', 'i', 'get', 'home', 'from', 'school', '.']\n",
      "['what', 'i', 'do', 'when', 'i', 'get', 'home', 'from', 'school', '.']\n",
      "['on', 'monday', 'i', 'sometimes', 'go', 'down', 'the', 'farm', 'in', 'the', 'night', 'i', 'wach', 'tv', 'there', 'is', 'bbc', 'and', 'i.t.v', '.']\n",
      "['on', 'monday', 'i', 'sometimes', 'go', 'down', 'the', 'farm', 'in', 'the', 'night', 'i', 'wash', 'tv', 'there', 'is', 'bbc', 'and', 'i.t.v', '.']\n",
      "['on', 'monday', 'i', 'sometimes', 'go', 'down', 'the', 'farm', 'in', 'the', 'night', 'i', 'watch', 'tv', 'there', 'is', 'bbc', 'and', 'i.t.v', '.']\n",
      "['i', 'like', 'i.t.v', '.']\n",
      "['i', 'like', 'i.t.v', '.']\n",
      "['i', 'like', 'i.t.v', '.']\n",
      "['we', 'call', 'anglia', 'i.t.v', '.']\n",
      "['we', 'call', 'anglia', 'i.t.v', '.']\n",
      "['we', 'call', 'anglia', 'i.t.v', '.']\n",
      "['we', 'have', 'got', 'anglia', 'like', 'to', 'wach', 'cow_boys', '.']\n",
      "['we', 'have', 'got', 'anglia', 'like', 'to', 'wash', 'cowboy', '.']\n",
      "['we', 'have', 'got', 'anglia', 'like', 'to', 'watch', 'cowboys', '.']\n",
      "['on', 'tuesday', 'i', 'get', 'off', 'the', 'bus', 'and', 'some_times', 'in', 'the', 'night', 'i', 'go', 'to', 'the', 'youth', 'colbe', '.']\n",
      "['on', 'tuesday', 'i', 'get', 'off', 'the', 'bus', 'and', 'sometimes', 'in', 'the', 'night', 'i', 'go', 'to', 'the', 'youth', 'coote', '.']\n",
      "['on', 'tuesday', 'i', 'get', 'off', 'the', 'bus', 'and', 'sometimes', 'in', 'the', 'night', 'i', 'go', 'to', 'the', 'youth', 'club', '.']\n",
      "['i', 'like', 'to', 'wach', 't.v', '.']\n",
      "['i', 'like', 'to', 'wash', 't.v', '.']\n",
      "['i', 'like', 'to', 'watch', 't.v', '.']\n",
      "['there', 'is', 'a', 'lot', 'of', 'things', 'on', 't.v', '.']\n",
      "['there', 'is', 'a', 'lot', 'of', 'things', 'on', 't.v', '.']\n",
      "['there', 'is', 'a', 'lot', 'of', 'things', 'on', 't.v', '.']\n",
      "['i', 'wach', 'it', 'each', 'night', '.']\n",
      "['i', 'wash', 'it', 'each', 'night', '.']\n",
      "['i', 'watch', 'it', 'each', 'night', '.']\n",
      "['.']\n",
      "['.']\n",
      "['.']\n",
      "['i', 'thing', 'tv', 'is', 'good', 'but', 'people', 'say', 'it', 'gives', 'us', 'squar', 'iyes', '.']\n",
      "['i', 'thing', 'tv', 'is', 'good', 'but', 'people', 'say', 'it', 'gives', 'us', 'squeak', 'eyes', '.']\n",
      "['i', 'think', 'tv', 'is', 'good', 'but', 'people', 'say', 'it', 'gives', 'us', 'square', 'eyes', '.']\n",
      "['the', 'murder', 'car', '.']\n",
      "['the', 'murder', 'car', '.']\n",
      "['the', 'murder', 'car', '.']\n",
      "['1', 'night', 'when', 'it', 'was', 'dark', 'about', '12', 'oclock', 'a', 'man', 'was', 'nock', 'down', 'by', 'a', 'car', '.']\n",
      "['1', 'night', 'when', 'it', 'was', 'dark', 'about', '12', \"o'clock\", 'a', 'man', 'was', 'rock', 'down', 'by', 'a', 'car', '.']\n",
      "['1', 'night', 'when', 'it', 'was', 'dark', 'about', '12', \"o'clock\", 'a', 'man', 'was', 'knocked', 'down', 'by', 'a', 'car', '.']\n",
      "['the', 'body', 'was', 'found', 'a', '7', 'oclock', 'in', 'the', 'morning', '.']\n",
      "['the', 'body', 'was', 'found', 'a', '7', \"o'clock\", 'in', 'the', 'morning', '.']\n",
      "['the', 'body', 'was', 'found', 'at', '7', \"o'clock\", 'in', 'the', 'morning', '.']\n",
      "['the', 'name', 'of', 'the', 'man', 'who', 'was', 'kild', 'was', 'jack', 'robbinson', 'he', 'has', 'black', 'hair', 'brown', 'eyes', 'blue', 'jacket', 'and', 'blue', 'jeans', '.']\n",
      "['the', 'name', 'of', 'the', 'man', 'who', 'was', 'kid', 'was', 'jack', 'robbinson', 'he', 'has', 'black', 'hair', 'brown', 'eyes', 'blue', 'jacket', 'and', 'blue', 'jeans', '.']\n",
      "['the', 'name', 'of', 'the', 'man', 'who', 'was', 'killed', 'was', 'jack', 'robbinson', 'he', 'has', 'black', 'hair', 'brown', 'eyes', 'blue', 'jacket', 'and', 'blue', 'jeans', '.']\n",
      "['no', 'one', 'see', 'the', 'car', '.']\n",
      "['no', 'one', 'see', 'the', 'car', '.']\n",
      "['no', 'one', 'saw', 'the', 'car', '.']\n",
      "['there', 'had', 'bean', 'a', 'lot', 'of', 'people', 'nock', 'down', 'by', 'a', 'car', 'so', 'they', 'cald', 'it', 'the', 'murder', 'car', '.']\n",
      "['there', 'had', 'beans', 'a', 'lot', 'of', 'people', 'rock', 'down', 'by', 'a', 'car', 'so', 'they', 'calm', 'it', 'the', 'murder', 'car', '.']\n",
      "['there', 'had', 'been', 'a', 'lot', 'of', 'people', 'knocked', 'down', 'by', 'a', 'car', 'so', 'they', 'called', 'it', 'the', 'murder', 'car', '.']\n",
      "['the', 'police', 'cam', 'out', 'to', 'look', 'for', 'the', 'car', '.']\n",
      "['the', 'police', 'ham', 'out', 'to', 'look', 'for', 'the', 'car', '.']\n",
      "['the', 'police', 'came', 'out', 'to', 'look', 'for', 'the', 'car', '.']\n",
      "['the', 'other', 'cars', 'were', 'black', 'so', 'one', 'of', 'the', 'other', 'people', 'said', '.']\n",
      "['the', 'other', 'cars', 'were', 'black', 'so', 'one', 'of', 'the', 'other', 'people', 'said', '.']\n",
      "['the', 'other', 'cars', 'were', 'black', 'so', 'one', 'of', 'the', 'other', 'people', 'said', '.']\n",
      "['the', 'murder', 'man', 'has', 'a', 'black', 'beard', 'the', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killd', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'hill', 'the', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "['the', 'murder', 'man', 'has', 'a', 'black', 'beard', 'the', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'kill', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'hill', 'the', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "['the', 'murder', 'man', 'has', 'a', 'black', 'beard', 'the', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killed', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'hill', 'there', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "['i', 'kissed', 'a', 'girl', 'one', 'night', 'here', 'iyes', 'were', 'burning', 'blue', 'she', 'said', 'o', 'do', 'you', 'love', 'me', 'of', 'course', 'of', 'course', 'i', 'do', '.']\n",
      "['i', 'kissed', 'a', 'girl', 'one', 'night', 'here', 'eyes', 'were', 'burning', 'blue', 'she', 'said', 'o', 'do', 'you', 'love', 'me', 'of', 'course', 'of', 'course', 'i', 'do', '.']\n",
      "['i', 'kissed', 'a', 'girl', 'one', 'night', 'her', 'eyes', 'were', 'burning', 'blue', 'she', 'said', 'o', 'do', 'you', 'love', 'me', 'of', 'course', 'of', 'course', 'i', 'do', '.']\n",
      "['bell', 'ringing', '.']\n",
      "['bell', 'ringing', '.']\n",
      "['bell', 'ringing', '.']\n",
      "['when', 'you', 'start', 'bell', 'ringing', 'you', 'haveto', 'ring', 'a', 'bell', 'be_for', 'you', 'can', 'do', 'any_thing', 'als', '.']\n",
      "['when', 'you', 'start', 'bell', 'ringing', 'you', 'have', 'ring', 'a', 'bell', 'before', 'you', 'can', 'do', 'anything', 'also', '.']\n",
      "['when', 'you', 'start', 'bell', 'ringing', 'you', 'have_to', 'ring', 'a', 'bell', 'before', 'you', 'can', 'do', 'anything', 'else', '.']\n",
      "['it', 'takes', 'a', 'lot', 'to', 'ring', 'a', 'bell', 'on', 'the', 'rope', 'there', 'is', 'a', 'sally', '.']\n",
      "['it', 'takes', 'a', 'lot', 'to', 'ring', 'a', 'bell', 'on', 'the', 'rope', 'there', 'is', 'a', 'sally', '.']\n",
      "['it', 'takes', 'a', 'lot', 'to', 'ring', 'a', 'bell', 'on', 'the', 'rope', 'there', 'is', 'a', 'sally', '.']\n",
      "['on', 'the', 'bell', 'there', 'is', 'a', 'weel', '.']\n",
      "['on', 'the', 'bell', 'there', 'is', 'a', 'weep', '.']\n",
      "['on', 'the', 'bell', 'there', 'is', 'a', 'wheel', '.']\n",
      "['the', 'weel', 'has', 'the', 'rope', 'on', 'it', 'some', 'sally', 'are', 'green', 'and', 'the', 'other', 'are', 'red', 'white', 'and', 'blue', '.']\n",
      "['the', 'weep', 'has', 'the', 'rope', 'on', 'it', 'some', 'sally', 'are', 'green', 'and', 'the', 'other', 'are', 'red', 'white', 'and', 'blue', '.']\n",
      "['the', 'wheel', 'has', 'the', 'rope', 'on', 'it', 'some', 'sallies', 'are', 'green', 'and', 'the', 'others', 'are', 'red', 'white', 'and', 'blue', '.']\n",
      "['when', 'you', 'can', 'ring', '.']\n",
      "['when', 'you', 'can', 'ring', '.']\n",
      "['when', 'you', 'can', 'ring', '.']\n",
      "['you', 'rings', 'like', 'this', '.']\n",
      "['you', 'wings', 'like', 'this', '.']\n",
      "['you', 'ring', 'like', 'this', '.']\n",
      "['these', 'are', 'the', 'names', 'of', 'them', 'plain', 'hunt', ',', 'plain', 'bob', ',', 'grandsire', 'doubles', '.']\n",
      "['these', 'are', 'the', 'names', 'of', 'them', 'plain', 'hunt', ',', 'plain', 'bob', ',', 'grandsire', 'doubles', '.']\n",
      "['these', 'are', 'the', 'names', 'of', 'them', 'plain', 'hunt', ',', 'plain', 'bob', ',', 'grandsire', 'doubles', '.']\n",
      "['grandsire', 'triples', 'it', 'takes', 'a', 'long', 'time', 'to', 'ring', 'them', '.']\n",
      "['grandsire', 'triples', 'it', 'takes', 'a', 'long', 'time', 'to', 'ring', 'them', '.']\n",
      "['grandsire', 'triples', 'it', 'takes', 'a', 'long', 'time', 'to', 'ring', 'them', '.']\n",
      "['me', 'and', 'my', 'dad', 'bike', 'to', 'melton', '.']\n",
      "['me', 'and', 'my', 'dad', 'bike', 'to', 'melton', '.']\n",
      "['me', 'and', 'my', 'dad', 'bike', 'to', 'melton', '.']\n",
      "['the', 'sick', 'sow', 'of', 'the', 'army', 'court']\n",
      "['the', 'sick', 'sow', 'of', 'the', 'army', 'court']\n",
      "['the', 'sick', 'sow', 'of', 'the', 'army', 'court']\n",
      "['one', 'day', 'sergent', 's', '.', 'm', '.', 'bullimore', 'told', 'hut', '29', 'to', 'clean', 'cynthia', \"'s\", 'pig', 'sty', 'out', '.']\n",
      "['one', 'day', 'sergent', 's', '.', 'm', '.', 'bullimore', 'told', 'hut', '29', 'to', 'clean', 'cynthia', \"'s\", 'pig', 'sty', 'out', '.']\n",
      "['one', 'day', 'sergent', 's', '.', 'm', '.', 'bullimore', 'told', 'hut', '29', 'to', 'clean', 'cynthia', \"'s\", 'pig', 'sty', 'out', '.']\n",
      "['when', 'hut', '29', 'got', 'there', 'they', 'had', 'to', 'go', 'and', 'get', 'some', 'gass', 'marsks', 'because', 'the', 'smell', 'was', 'to', 'strong']\n",
      "['when', 'hut', '29', 'got', 'there', 'they', 'had', 'to', 'go', 'and', 'get', 'some', 'mass', 'mass', 'because', 'the', 'smell', 'was', 'to', 'strong']\n",
      "['when', 'hut', '29', 'got', 'there', 'they', 'had', 'to', 'go', 'and', 'get', 'some', 'gas', 'masks', 'because', 'the', 'smell', 'was', 'to', 'strong']\n",
      "['when', 's', 'm', 'bullimore', 'came', 'to', 'the', 'pig', 'sty', 'the', 'pig', 'was', 'laid', 'out', 'on', 'the', 'foor']\n",
      "['when', 's', 'm', 'bullimore', 'came', 'to', 'the', 'pig', 'sty', 'the', 'pig', 'was', 'laid', 'out', 'on', 'the', 'poor']\n",
      "['when', 's', 'm', 'bullimore', 'came', 'to', 'the', 'pig', 'sty', 'the', 'pig', 'was', 'laid', 'out', 'on', 'the', 'floor']\n",
      "['when', 'they', 'came', 'back', 's', '.', 'm', '.', 'bullimore', 'said', 'wat', 'have', 'you', 'been', 'doing', 'to', 'my', 'sow']\n",
      "['when', 'they', 'came', 'back', 's', '.', 'm', '.', 'bullimore', 'said', 'wad', 'have', 'you', 'been', 'doing', 'to', 'my', 'sow']\n",
      "['when', 'they', 'came', 'back', 's', '.', 'm', '.', 'bullimore', 'said', 'what', 'have', 'you', 'been', 'doing', 'to', 'my', 'sow']\n",
      "['we', 'have', 'not', 'done', 'anything', 'we', 'had', 'to', 'go', 'and', 'get', 'some', 'gass', 'marsks', 'because', 'the', 'smell', 'was', 'to', 'strong']\n",
      "['we', 'have', 'not', 'done', 'anything', 'we', 'had', 'to', 'go', 'and', 'get', 'some', 'mass', 'mass', 'because', 'the', 'smell', 'was', 'to', 'strong']\n",
      "['we', 'have', 'not', 'done', 'anything', 'we', 'had', 'to', 'go', 'and', 'get', 'some', 'gas', 'masks', 'because', 'the', 'smell', 'was', 'too', 'strong']\n",
      "['that', 'is', 'only', 'the', 'soap', 'wat', 'i', 'wosh', 'it', 'with']\n",
      "['that', 'is', 'only', 'the', 'soap', 'wad', 'i', 'wash', 'it', 'with']\n",
      "['that', 'is', 'only', 'the', 'soap', 'what', 'i', 'wash', 'it', 'with']\n",
      "['prasp', 'he', 'et', 'some', 'soap']\n",
      "['pass', 'he', 'pet', 'some', 'soap']\n",
      "['perhaps', 'he', 'ate', 'some', 'soap']\n",
      "['flogger', 'said', 'shall', 'i', 'go', 'and', 'get', 'the', 'vet']\n",
      "['flogger', 'said', 'shall', 'i', 'go', 'and', 'get', 'the', 'vet']\n",
      "['flogger', 'said', 'shall', 'i', 'go', 'and', 'get', 'the', 'vet']\n",
      "['get', 'back', 'in', 'the', 'line', 'be_for', 'i', 'put', 'you', 'on', 'charge']\n",
      "['get', 'back', 'in', 'the', 'line', 'before', 'i', 'put', 'you', 'on', 'charge']\n",
      "['get', 'back', 'in', 'the', 'line', 'before', 'i', 'put', 'you', 'on', 'charge']\n",
      "['what', 'did', 'you', 'say']\n",
      "['what', 'did', 'you', 'say']\n",
      "['what', 'did', 'you', 'say']\n",
      "['o', 'nothing', 'sir']\n",
      "['o', 'nothing', 'sir']\n",
      "['o', 'nothing', 'sir']\n",
      "['go', 'and', 'get', 'some', 'wiskey']\n",
      "['go', 'and', 'get', 'some', 'whiskey']\n",
      "['go', 'and', 'get', 'some', 'whiskey']\n",
      "['okay', 'sir']\n",
      "['okay', 'sir']\n",
      "['okay', 'sir']\n",
      "['put', 'some', 'soap', 'in', 'it', 'flogger', 'said', 'oswald']\n",
      "['put', 'some', 'soap', 'in', 'it', 'flogger', 'said', 'oswald']\n",
      "['put', 'some', 'soap', 'in', 'it', 'flogger', 'said', 'oswald']\n",
      "['ok', 'said', 'flogger']\n",
      "['ok', 'said', 'flogger']\n",
      "['ok', 'said', 'flogger']\n",
      "['flogger', 'went', 'off']\n",
      "['flogger', 'went', 'off']\n",
      "['flogger', 'went', 'off']\n",
      "['a', 'little', 'while', 'after', 'he', 'came', 'with', 'a', 'large', 'soape', 'wiskey']\n",
      "['a', 'little', 'while', 'after', 'he', 'came', 'with', 'a', 'large', 'sore', 'whiskey']\n",
      "['a', 'little', 'while', 'after', 'he', 'came', 'with', 'a', 'large', 'soap', 'whiskey']\n",
      "['he', 'cave', 'it', 'to', 's', '.', 'm', '.', 'bullimore']\n",
      "['he', 'cage', 'it', 'to', 's', '.', 'm', '.', 'bullimore']\n",
      "['he', 'gave', 'it', 'to', 's', '.', 'm', '.', 'bullimore']\n",
      "['then', 'he', 'cave', 'it', 'to', 'the', 'pig']\n",
      "['then', 'he', 'cage', 'it', 'to', 'the', 'pig']\n",
      "['then', 'he', 'gave', 'it', 'to', 'the', 'pig']\n",
      "['a', 'little', 'while', 'after', 'the', 'pig', 'go', 'up', 'and', 'there', 'beside', 'here', 'were', 'six', 'little', 'pigs']\n",
      "['a', 'little', 'while', 'after', 'the', 'pig', 'go', 'up', 'and', 'there', 'beside', 'here', 'were', 'six', 'little', 'pigs']\n",
      "['a', 'little', 'while', 'after', 'the', 'pig', 'got', 'up', 'and', 'there', 'beside', 'her', 'were', 'six', 'little', 'pigs']\n",
      "['s', 'm', 'bullimore', 'said', 'that', 'made', 'here', 'better', 'that', 'was', 'not', 'me', 'that', 'was', 'the', 'sope']\n",
      "['s', 'm', 'bullimore', 'said', 'that', 'made', 'here', 'better', 'that', 'was', 'not', 'me', 'that', 'was', 'the', 'sore']\n",
      "['s', 'm', 'bullimore', 'said', 'that', 'made', 'her', 'better', 'that', 'was', 'not', 'me', 'that', 'was', 'the', 'soap']\n",
      "['you', 'are', 'on', 'charge']\n",
      "['you', 'are', 'on', 'charge']\n",
      "['you', 'are', 'on', 'charge']\n",
      "['wat', 'did', 'you', 'give', 'here']\n",
      "['wad', 'did', 'you', 'give', 'here']\n",
      "['what', 'did', 'you', 'give', 'her']\n",
      "['i', 'cave', 'her', 'some', 'wiskey', 'and', 'sope', 'you', 'cave', 'her', 'that']\n",
      "['i', 'cage', 'her', 'some', 'whiskey', 'and', 'sore', 'you', 'cage', 'her', 'that']\n",
      "['i', 'gave', 'her', 'some', 'whiskey', 'and', 'soap', 'you', 'gave', 'her', 'that']\n",
      "['no', 'i', 'knver', 'i', 'sir']\n",
      "['no', 'i', 'knees', 'i', 'sir']\n",
      "['no', 'i', 'never', 'i', 'sir']\n",
      "['you', 'did']\n",
      "['you', 'did']\n",
      "['you', 'did']\n",
      "['thats', \"'s\", 'one', 'thing', 'sir', 'it', 'has', 'made', 'her', 'better', '.']\n",
      "['that', \"'s\", 'one', 'thing', 'sir', 'it', 'has', 'made', 'her', 'better', '.']\n",
      "['that', \"'s\", 'one', 'thing', 'sir', 'it', 'has', 'made', 'her', 'better', '.']\n",
      "['pigs', 'when', 'young', 'up', 'to', 'being', 'kild', 'for', 'bacon']\n",
      "['pigs', 'when', 'young', 'up', 'to', 'being', 'kid', 'for', 'bacon']\n",
      "['pigs', 'when', 'young', 'up', 'to', 'being', 'killed', 'for', 'bacon']\n",
      "['when', 'they', 'are', 'young', 'you', 'have', 'to', 'wate', '3', 'days', 'then', 'you', 'can', 'injeck', 'them', 'for', 'pneumonia', 'diseases']\n",
      "['when', 'they', 'are', 'young', 'you', 'have', 'to', 'tate', '3', 'days', 'then', 'you', 'can', 'neck', 'them', 'for', 'pneumonia', 'diseases']\n",
      "['when', 'they', 'are', 'young', 'you', 'have', 'to', 'wait', '3', 'days', 'then', 'you', 'can', 'inject', 'them', 'for', 'pneumonia', 'diseases']\n",
      "['you', 'have', 'mack', 'shore', 'they', 'have', 'drye', 'straw']\n",
      "['you', 'have', 'mask', 'shore', 'they', 'have', 'dry', 'straw']\n",
      "['you', 'have', 'make', 'sure', 'they', 'have', 'dry', 'straw']\n",
      "['when', 'you', 'clean', 'them', 'out', 'you', 'should', 'not', 'leave', 'a', 'falk', 'in', 'with', 'them', 'because', 'the', 'mother', 'might', 'nock', 'it', 'down', 'and', 'the', 'little', 'pigs', 'might', 'stab', 'them_souve']\n",
      "['when', 'you', 'clean', 'them', 'out', 'you', 'should', 'not', 'leave', 'a', 'fall', 'in', 'with', 'them', 'because', 'the', 'mother', 'might', 'rock', 'it', 'down', 'and', 'the', 'little', 'pigs', 'might', 'stab', 'themselves']\n",
      "['when', 'you', 'clean', 'them', 'out', 'you', 'should', 'not', 'leave', 'a', 'fork', 'in', 'with', 'them', 'because', 'the', 'mother', 'might', 'knock', 'it', 'down', 'and', 'the', 'little', 'pigs', 'might', 'stab', 'themselves']\n",
      "['we', 'give', 'the', 'worme', 'pouder']\n",
      "['we', 'give', 'the', 'worms', 'wonder']\n",
      "['we', 'give', 'them', 'worm', 'powder']\n",
      "['that', 'is', 'when', 'they', 'get', 'the', 'worme']\n",
      "['that', 'is', 'when', 'they', 'get', 'the', 'worms']\n",
      "['that', 'is', 'when', 'they', 'get', 'the', 'worm']\n",
      "['this', 'will', 'stop', 'them', 'from', 'going', 'thin']\n",
      "['this', 'will', 'stop', 'them', 'from', 'going', 'thin']\n",
      "['this', 'will', 'stop', 'them', 'from', 'going', 'thin']\n",
      "['you', 'should', 'box', 'a', 'little', 'place', 'off', 'so', 'only', 'the', 'little', 'pigs', 'can', 'get', 'in', 'it']\n",
      "['you', 'should', 'box', 'a', 'little', 'place', 'off', 'so', 'only', 'the', 'little', 'pigs', 'can', 'get', 'in', 'it']\n",
      "['you', 'should', 'box', 'a', 'little', 'place', 'off', 'so', 'only', 'the', 'little', 'pigs', 'can', 'get', 'in', 'it']\n",
      "['that', 'is', 'so', 'they', 'can', 'ge', 'out', 'of', 'the', 'way', 'of', 'there', 'mother']\n",
      "['that', 'is', 'so', 'they', 'can', 'age', 'out', 'of', 'the', 'way', 'of', 'there', 'mother']\n",
      "['that', 'is', 'so', 'they', 'can', 'get', 'out', 'of', 'the', 'way', 'of', 'their', 'mother']\n",
      "['some', 'people', 'put', 'a', 'light', 'in', 'with', 'theme', 'to', 'geep', 'them', 'warm']\n",
      "['some', 'people', 'put', 'a', 'light', 'in', 'with', 'these', 'to', 'weep', 'them', 'warm']\n",
      "['some', 'people', 'put', 'a', 'light', 'in', 'with', 'them', 'to', 'keep', 'them', 'warm']\n",
      "['you', 'have', 'to', 'make', 'shore', 'that', 'mother', 'has', 'a', 'lot', 'of', 'milk', '.']\n",
      "['you', 'have', 'to', 'make', 'shore', 'that', 'mother', 'has', 'a', 'lot', 'of', 'milk', '.']\n",
      "['you', 'have', 'to', 'make', 'sure', 'that', 'mother', 'has', 'a', 'lot', 'of', 'milk', '.']\n",
      "['if', 'she', 'as', 'not', 'got', 'a_nougth', 'milk', 'you', 'will', 'have', 'to', 'feed', 'them', 'on', 'a', 'bottle']\n",
      "['if', 'she', 'as', 'not', 'got', 'enough', 'milk', 'you', 'will', 'have', 'to', 'feed', 'them', 'on', 'a', 'bottle']\n",
      "['if', 'she', 'has', 'not', 'got', 'enough', 'milk', 'you', 'will', 'have', 'to', 'feed', 'them', 'on', 'a', 'bottle']\n",
      "['when', 'they', 'came', 'eat', 'a', 'little', 'bit', 'you', 'can', 'get', 'them', 'some', 'little', 'nuts', 'of', 'fating']\n",
      "['when', 'they', 'came', 'eat', 'a', 'little', 'bit', 'you', 'can', 'get', 'them', 'some', 'little', 'nuts', 'of', 'making']\n",
      "['when', 'they', 'can', 'eat', 'a', 'little', 'bit', 'you', 'can', 'get', 'them', 'some', 'little', 'nuts', 'for', 'fattening']\n",
      "['they', 'can', 'eate', 'some', 'meal', 'when', 'they', 'get', 'a', 'little', 'biger']\n",
      "['they', 'can', 'eats', 'some', 'meal', 'when', 'they', 'get', 'a', 'little', 'bigger']\n",
      "['they', 'can', 'eat', 'some', 'meal', 'when', 'they', 'get', 'a', 'little', 'bigger']\n",
      "['we', 'give', 'them', 'some', 'fating', 'food', 'called', 'nomber', '2']\n",
      "['we', 'give', 'them', 'some', 'falling', 'food', 'called', 'nomber', '2']\n",
      "['we', 'give', 'them', 'some', 'fattening', 'food', 'called', 'nomber', '2']\n",
      "['whe', 'you', 'wean', 'that', 'is', 'take', 'them', 'from', 'their', 'mother', 'you', 'have', 'got', 'to', 'see', 'if', 'they', 'fight', 'if', 'there', 'is', 'any', 'little', 'wones']\n",
      "['why', 'you', 'wean', 'that', 'is', 'take', 'them', 'from', 'their', 'mother', 'you', 'have', 'got', 'to', 'see', 'if', 'they', 'fight', 'if', 'there', 'is', 'any', 'little', 'jones']\n",
      "['when', 'you', 'wean', 'that', 'is', 'take', 'them', 'from', 'their', 'mother', 'you', 'have', 'got', 'to', 'see', 'if', 'they', 'fight', 'if', 'there', 'is', 'any', 'little', 'ones']\n",
      "['fighting']\n",
      "['fighting']\n",
      "['fighting']\n",
      "['when', 'they', 'and', 'bing', 'you', 'have', 'to', 'see', 'about', 'waying']\n",
      "['when', 'they', 'and', 'king', 'you', 'have', 'to', 'see', 'about', 'saying']\n",
      "['when', 'they', 'are', 'big', 'you', 'have', 'to', 'see', 'about', 'weighing']\n",
      "['when', 'they', 'have', 'been', 'waid', 'and', 'reddy', 'to', 'go', 'away', 'to', 'be', 'kild', '.']\n",
      "['when', 'they', 'have', 'been', 'wad', 'and', 'ready', 'to', 'go', 'away', 'to', 'be', 'kid', '.']\n",
      "['when', 'they', 'have', 'been', 'weighed', 'and', 'ready', 'to', 'go', 'away', 'to', 'be', 'killed', '.']\n",
      "['billy', 'bunter', 'was', 'to', 'large', 'so', 'they', 'sent', 'for', 'im', 'in', 'charge']\n",
      "['billy', 'bunter', 'was', 'to', 'large', 'so', 'they', 'sent', 'for', 'kim', 'in', 'charge']\n",
      "['billy', 'bunter', 'was', 'too', 'large', 'so', 'they', 'sent', 'for', 'him', 'in', 'charge']\n",
      "['im', 'in', 'charge', 'was', 'to', 'thin', 'so', 'they', 'sent', 'fro', 'rin', 'tin', 'tin']\n",
      "['kim', 'in', 'charge', 'was', 'to', 'thin', 'so', 'they', 'sent', 'frog', 'rin', 'tin', 'tin']\n",
      "['him', 'in', 'charge', 'was', 'too', 'thin', 'so', 'they', 'sent', 'for', 'rin', 'tin', 'tin']\n",
      "['rin', 'tin', 'tin', 'heart', 'is', 'poor', 'so', 'they', 'sent', 'for', 'barbra', 'moore']\n",
      "['rin', 'tin', 'tin', 'heart', 'is', 'poor', 'so', 'they', 'sent', 'for', 'barbra', 'moore']\n",
      "['rin', 'tin', 'tin', 'heart', 'is', 'poor', 'so', 'they', 'sent', 'for', 'barbra', 'moore']\n",
      "['barbra', 'moore', 'was', 'having', 'dinner', 'so', 'the', 'sent', 'for', 'yule', 'brinner']\n",
      "['barbra', 'moore', 'was', 'having', 'dinner', 'so', 'the', 'sent', 'for', 'yule', 'brinner']\n",
      "['barbra', 'moore', 'was', 'having', 'dinner', 'so', 'they', 'sent', 'for', 'yule', 'brinner']\n",
      "['yule', 'brinner', 'sang', 'to', 'high', 'then', 'they', 'went', 'to', 'space', 'in', 'the', 'sky']\n",
      "['yule', 'brinner', 'sang', 'to', 'high', 'then', 'they', 'went', 'to', 'space', 'in', 'the', 'sky']\n",
      "['yule', 'brinner', 'sang', 'too', 'high', 'then', 'they', 'went', 'to', 'space', 'in', 'the', 'sky']\n",
      "['my', 'heart', 'is', 'full', 'of', 'sadness', 'my', 'heart', 'is', 'full', 'of', 'joy', 'it', 'might', 'be', 'my', 'wife', 'or', 'it', 'might', 'be', 'helen', 'of', 'toy']\n",
      "['my', 'heart', 'is', 'full', 'of', 'sadness', 'my', 'heart', 'is', 'full', 'of', 'joy', 'it', 'might', 'be', 'my', 'wife', 'or', 'it', 'might', 'be', 'helen', 'of', 'tom']\n",
      "['my', 'heart', 'is', 'full', 'of', 'sadness', 'my', 'heart', 'is', 'full', 'of', 'joy', 'it', 'might', 'be', 'my', 'wife', 'or', 'it', 'might', 'be', 'helen', 'of', 'troy']\n",
      "['one', 'saturday', 'i', 'though', 'i', 'would', 'go', 'to', 'the', 'races', 'at', 'london']\n",
      "['one', 'saturday', 'i', 'trough', 'i', 'would', 'go', 'to', 'the', 'races', 'at', 'london']\n",
      "['one', 'saturday', 'i', 'thought', 'i', 'would', 'go', 'to', 'the', 'races', 'at', 'london']\n",
      "['i', 'went', 'on', 'my', 'royl', 'enfield']\n",
      "['i', 'went', 'on', 'my', 'roy', 'enfield']\n",
      "['i', 'went', 'on', 'my', 'royal', 'enfield']\n",
      "['they', 'can', 'go', 'quite', 'farst']\n",
      "['they', 'can', 'go', 'quite', 'fast']\n",
      "['they', 'can', 'go', 'quite', 'fast']\n",
      "['this', 'was', 'a', 'royl', 'enfield', 'consulatoin', '?', '_']\n",
      "['this', 'was', 'a', 'roy', 'enfield', 'counting', '?', '_']\n",
      "['this', 'was', 'a', 'royal', 'enfield', '_', '?', '_']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data_test_incorrect)):\n",
    "    print(data_test_incorrect[i])\n",
    "    print(predictions[i])\n",
    "    print(data_test_correct[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## **Task 6:**\n",
    "\n",
    "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3\n",
    "\n",
    "_Marks will be awarded based on how interesting the proposed improvement is. Please provide a short text describing what you intend to do and why. Full marks for this section may be obtained without an implementation, but an implementation is preferred._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 6 description **\n",
    "\n",
    "Previuously, i saw that the incorrect words which are only at 1 edit distance away are calculated and \n",
    "candidates are returned due to the calculation of edit distance that we had to incorporate in task 3. Firstly,\n",
    "i decided to change the calculation of candidates via edit distance to include even candidates at 1 or 2 more distances \n",
    "compared to just minimum edit distance. \n",
    "Secondly, i thought of incorporating trigrams in task 6 instead of bigrams to provide better context to my algorithm and \n",
    "create less confusion in selecting the right candidate.\n",
    "To calculate trigrams probability, i extended the previous formulae and read documents online for trigrams probability \n",
    "calculation formulae.\n",
    "\n",
    "Incorporation of trigrams:\n",
    "I check if the word of the sentence is not in first 2 indices and last two indices, extract the previous 2 \n",
    "and following 2 words. After that the probability of the candidate is calculated as:\n",
    "    P(a particular candidate  say, w) = P(seeing previous 2 words and w|previous 2 words) * \n",
    "                                            P( seeing w followed by the next two words| w)\n",
    "\n",
    "Similarly, now I had to tackle words in first and last indices as well as 2nd and 2nd last indices.\n",
    "For words in first and last indices, we take the following two words and previous 2 words respectively for calculation \n",
    "of the trigram probabilities. Also, here the length of the sentence needs to be taken into account that it is atleast 3\n",
    "so as to get the follwing and previosu words.\n",
    "Unlike in bigrams, here we need to handle probabilities for 2nd and 2nd last indices separately as well. For these, we take the previous one word available and follwing two words after candidate to calculate probability.\n",
    "\n",
    "For sentences with length 2, previous function defined in 4 to correct sentences was used and bigram probabilities considered\n",
    "\n",
    "\n",
    "* Further ideas I worked at but didn't work out:\n",
    "1) Considered removing stopwords from both tokens of correct words in dictionary and test incorrect and correct\n",
    "2) Implemented that later but the problem encountered there is that the length of test_correct and incorrect sentences are not the same. This is due to the case that in correct sentences list, words such as been, have etc are removed whereas in incorrect, if the spellings are wrong; like I saw bean instead of been, hador instead of have; those words are not removed due to which length differs.\n",
    "And thus, this second idea i could not further work on. Thanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above, looking at incorrect, predicted and corrected sentences I saw that the algorithm was considering candidates\n",
    "# mostly only one distance away words, for instance; wach becomes wash; watch isn't even considered. So, for task 6, \n",
    "# i decided  to look at not only the minimum edit distance which might be 1 in most cases, but also to look at 1 more change,\n",
    "# that is if a word is within 2 edit distances, we consider it as a candidate\n",
    "\n",
    "\n",
    "\n",
    "# Firstly, create new get_candidates function which also considers edit distance at min_edit_distance +1\n",
    "# and then instead of bigram, I have looked here at trigram probabilities so that the algorithm has more context\n",
    "def get_candidates_impr(token):\n",
    "    result = []\n",
    "    min_edit_dist = 100 # initializing edit distance to a high number at first\n",
    "    \n",
    "    # For all words in set of unique_tokens, i check edit distance with input token\n",
    "    for word in unique_tokens:\n",
    "        current_dist = edit_distance(token, word)\n",
    "\n",
    "        # changing the minimum edit distance found\n",
    "        if(current_dist < min_edit_dist):\n",
    "            min_edit_dist = current_dist\n",
    "            \n",
    "    # appending all candidates to a list and returning that list\n",
    "    for word in unique_tokens:\n",
    "        \n",
    "        # Here, i made change considering words which are 1 more than min_edit_distance also to check if wach\n",
    "        # can be converted to watch instead of wash\n",
    "        if(edit_distance(token, word) in(min_edit_dist, min_edit_dist + 1)):\n",
    "            result.append(word)\n",
    "    return (result)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Instead of looking at bigrams to calculate probability, I thought looking at trigrams should give more context and less of\n",
    "# confusion to algorithm while calculating conditional probability. For calculating conditional probability of trigrams, \n",
    "# I have used extended version of formulae given above. \n",
    "# To calculate probability\n",
    "def correct_impr(s, index_list):\n",
    "    correct = []\n",
    "    sentence = []\n",
    "    for i in s:\n",
    "        sentence.append(i)\n",
    "\n",
    "# for each word in sentence, here i check if index of word is not in the index list of incorrect indexes, only then do i \n",
    "# append the word to the corrected sentence otherwise, cadidates are calculated for it with minimum edit distance,\n",
    "# and distance + 1 so as to capture meaningful candidates such as watch for wach and goes for go\n",
    "    for i in sentence:\n",
    "        if (sentence.index(i) not in index_list):\n",
    "            correct.append(i)\n",
    "        else:\n",
    "            candidates = list(set(get_candidates_impr(i)))\n",
    "            \n",
    "            # words after 1st index and before 2nd last word\n",
    "            if(sentence.index(i) > 1 and sentence.index(i) < (len(sentence) - 2)):\n",
    "                ind_word = sentence.index(i)\n",
    "                correct.append(trigram_prob(candidates, ind_word, sentence))\n",
    "            \n",
    "            # first and last word\n",
    "            elif((sentence.index(i) == 0 and len(sentence) > 2) or (sentence.index(i) == (len(sentence) - 1) \n",
    "                                                                     and len(sentence) > 2)):\n",
    "                index = sentence.index(i)\n",
    "                if(index == 0):\n",
    "                    foll_words = [sentence[1], sentence[2]]\n",
    "                    correct.append(fl_trigram_prob(candidates,foll_words,index))\n",
    "                else:\n",
    "                    prec_words = [sentence[len(sentence)-2], sentence[len(sentence)-1]]\n",
    "                    correct.append(fl_trigram_prob(candidates,prec_words,index))\n",
    "            \n",
    "            # for 1 index and 2nd last word\n",
    "            elif((sentence.index(i) == 1 or sentence.index(i) == (len(sentence) - 2)) and len(sentence) >= 3):\n",
    "                index = sentence.index(i)\n",
    "                correct.append(trigram_prob(candidates,index, sentence))\n",
    "            \n",
    "            # if the length of sentence is 2, i call on correct() to calculate bigram probability and append right candidate\n",
    "            elif(len(sentence) == 2):\n",
    "                correct.append(correct(sentence))\n",
    "\n",
    "    return correct\n",
    "        \n",
    "\n",
    "def trigram_prob(candidates,index, sentence):\n",
    "    cond_prob = []\n",
    "    \n",
    "    # If the word is in 1 index and length of sentence is greater than or equal to 3 in order to form trigrams\n",
    "    if (index == 1 and len(sentence) >= 3):\n",
    "        prec_word = sentence[0]\n",
    "        foll_words = [sentence[2], sentence[3]]\n",
    "        for i in candidates:\n",
    "            \n",
    "            # 0.0001- a very small number is added to both numerator and denominator to avoid dividing by 0 isuues\n",
    "            cond_prob.append(((calc_freq_bigram(prec_word, i) +0.0001)/(unigram(prec_word) +0.0001)) * ((calc_freq(i,foll_words) + 0.0001)/(unigram(i) + 0.0001)))\n",
    "        max_prob_index = cond_prob.index(max(cond_prob))\n",
    "        \n",
    "        # if the word is in 2nd last index\n",
    "    elif(index == len(sentence) -2):\n",
    "        prec_words = [sentence[len(sentence) - 4], sentence[len(sentence) - 3]]\n",
    "        foll_word = sentence[len(sentence) - 1]\n",
    "        for i in candidates:\n",
    "            cond_prob.append(((calc_freq(prec_words,i) +0.0001)/(calc_freq_bigram(prec_words[0],prec_words[1]) + 0.0001))*((calc_freq_bigram(i, foll_word) + 0.0001)/(unigram(i) + 0.01)))      \n",
    "            max_prob_index = cond_prob.index(max(cond_prob)) \n",
    "            \n",
    "        # for all indices above 2nd word and lesser than 2nd last word\n",
    "    elif(index > 1 and index < len(sentence) - 2 ):\n",
    "        prec = [sentence[index-2], sentence[index-1]]\n",
    "        foll = [sentence[index+1], sentence[index+2]]\n",
    "        for j in candidates:\n",
    "            cond_prob.append((calc_freq(prec,j) +0.0001)/(calc_freq_bigram(prec[0], prec[1]) + 0.0001)*(calc_freq(j, foll) + 0.0001)/(unigram(j) + 0.0001))\n",
    "        max_prob_index = cond_prob.index(max(cond_prob))\n",
    "    return candidates[max_prob_index]\n",
    "    \n",
    "\n",
    "# function defined to calculate first and last words trigram probabilities\n",
    "def fl_trigram_prob(candidates, words, index):\n",
    "    cond_prob = []\n",
    "    for j in candidates:\n",
    "        if(index == 0):\n",
    "            cond_prob.append((calc_freq(j, words) + 0.0001)/(unigram(j) + 0.0001 ))\n",
    "        else:\n",
    "            cond_prob.append((calc_freq(words,j) + 0.0001)/(calc_freq_bigram(words[0], words[1]) + 0.0001))\n",
    "    max_prob_index = cond_prob.index(max(cond_prob))\n",
    "    return candidates[max_prob_index]\n",
    "\n",
    "# Calculates frequency of trigrams and returns the count\n",
    "def calc_freq(prec_words, foll_words):\n",
    "    count = 0\n",
    "    trigrams = nltk.trigrams(correct_tokens)\n",
    "    if(len(prec_words) == 2):\n",
    "            words = prec_words\n",
    "            candidate = foll_words\n",
    "            for t in trigrams:\n",
    "                if(t[0] == words[0] and t[1] == words[1] and t[2] == candidate):\n",
    "                    count = count + 1\n",
    "    else:\n",
    "        words = foll_words\n",
    "        candidate = prec_words\n",
    "        for t in trigrams:            \n",
    "            if ([t[0]] == candidate and [b[1]]== words[0] and b[2] == words[1]):\n",
    "                count = count + 1\n",
    "    return count\n",
    "# calculates frequencies of bigrams\n",
    "def calc_freq_bigram(prec, foll):\n",
    "    count = 0\n",
    "    bigrams = nltk.bigrams(correct_tokens)\n",
    "    for b in bigrams:\n",
    "        if (b[0] == prec and b[1]== foll):\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "# I tried to incorporate edit distance of 2 words higher than minimum; but the time for execution of code takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other tryouts for task 6\n",
    "### change correct and incorrect sentences as well as indexes for stopwords\n",
    "## Here is where i thought of removing stopwords and checking algorithms performance, but stopwords removal for incorrect\n",
    "# sentences dont happen well due to wrong spellings of bean instead of been etc. Thus, this wasn't fully implemented\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "    \n",
    "# improve_tokens = [word for word in correct_tokens if word not in stop]\n",
    "\n",
    "data_test_correct\n",
    "data_test_incorrect\n",
    "test_corr = defaultdict(list)\n",
    "test_incorr = defaultdict(list)\n",
    "for i in range(len(data_test_correct)):\n",
    "    for j in range(len(data_test_correct[i])):\n",
    "        if(data_test_correct[i][j] not in stop and data_test_correct[i][j] != \"others\"):\n",
    "            test_corr[i].append(data_test_correct[i][j])\n",
    "            \n",
    "for i in range(len(data_test_incorrect)):\n",
    "    for j in range(len(data_test_incorrect[i])):\n",
    "        if(data_test_incorrect[i][j] not in stop):\n",
    "            test_incorr[i].append(data_test_incorrect[i][j])\n",
    "            \n",
    "            \n",
    "            \n",
    "# new_index = defaultdict(list)            \n",
    "# # To store new index\n",
    "# j=0\n",
    "# for i in range(len(test_corr)):\n",
    "#     if (test_corr[i] != test_incorr[i]):\n",
    "#         new_index[j].append(i)\n",
    "#         j = j+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 7:**\n",
    "\n",
    "Repeat the evaluation of your new algorithm and show that it outperforms the algorithm from Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.42993907745866\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "def accuracy_new(test):\n",
    "    count = 0\n",
    "    for j in range(len(data_test_incorrect)):\n",
    "        predictions.append(correct_impr(data_test_incorrect[j], index_test[j]))\n",
    "#         print(j)\n",
    "    p = list(chain.from_iterable(predictions))\n",
    "    d = list(chain.from_iterable(data_test_correct))               \n",
    "    for i in range(len(p)):  \n",
    "        if(p[i] == d[i]):\n",
    "            count = count +1\n",
    "    \n",
    "    return ((count/len(p)) * 100)\n",
    "  \n",
    "\n",
    "# or len(data_test_incorrect[j]) == 4or len(data_test_incorrect[j]) == 3 \n",
    "print(accuracy_new(data_test_incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '.']\n",
      "['1', '.']\n",
      "['1', '.']\n",
      "['nigel', 'thrush', 'page', '48']\n",
      "['nigel', 'thrush', 'page', '48']\n",
      "['nigel', 'thrush', 'page', '48']\n",
      "['i', 'have', 'four', 'in', 'my', 'family', 'dad', 'mum', 'and', 'siter', '.']\n",
      "['i', 'have', 'four', 'in', 'my', 'family', 'dad', 'mum', 'and', 'sister', '.']\n",
      "['i', 'have', 'four', 'in', 'my', 'family', 'dad', 'mum', 'and', 'sister', '.']\n",
      "['my', 'dad', 'works', 'at', 'melton', '.']\n",
      "['my', 'dad', 'works', 'at', 'melton', '.']\n",
      "['my', 'dad', 'works', 'at', 'melton', '.']\n",
      "['my', 'siter', 'go', 'to', 'tonbury', '.']\n",
      "['my', 'sister', 'wo', 'to', 'tonbury', '.']\n",
      "['my', 'sister', 'goes', 'to', 'tonbury', '.']\n",
      "['my', 'mum', 'goes', 'out', 'some_times', '.']\n",
      "['my', 'mum', 'goes', 'out', 'sometimes', '.']\n",
      "['my', 'mum', 'goes', 'out', 'sometimes', '.']\n",
      "['i', 'go', 'to', 'bridgebrook', 'i', 'go', 'out', 'some_times', 'on', 'tuesday', 'night', 'i', 'go', 'to', 'youth', 'clob', '.']\n",
      "['i', 'go', 'to', 'bridgebrook', 'i', 'go', 'out', 'sometimes', 'on', 'tuesday', 'night', 'i', 'go', 'to', 'youth', 'club', '.']\n",
      "['i', 'go', 'to', 'bridgebrook', 'i', 'go', 'out', 'sometimes', 'on', 'tuesday', 'night', 'i', 'go', 'to', 'youth', 'club', '.']\n",
      "['on', 'thursday', 'nights', 'i', 'go', 'bell_ringing', 'on', 'saturdays', 'i', 'go', 'down', 'to', 'the', 'farm', '.']\n",
      "['on', 'thursday', 'nights', 'i', 'go', 'binding', 'on', 'saturdays', 'i', 'go', 'down', 'to', 'the', 'farm', '.']\n",
      "['on', 'thursday', 'nights', 'i', 'go', 'bellringing', 'on', 'saturdays', 'i', 'go', 'down', 'to', 'the', 'farm', '.']\n",
      "['on', 'sundays', 'i', 'go', 'to', 'church', '.']\n",
      "['on', 'sundays', 'i', 'go', 'to', 'church', '.']\n",
      "['on', 'sundays', 'i', 'go', 'to', 'church', '.']\n",
      "['i', 'go', 'to', 'bed', 'at', '10', 'o', 'clock', 'i', 'wakh', 'tv', 'at', '5', 'o', 'clock', 'i', 'live', 'in', 'a', 'house', '.']\n",
      "['i', 'go', 'to', 'bed', 'at', '10', 'o', 'clock', 'i', 'watch', 'tv', 'at', '5', 'o', 'clock', 'i', 'live', 'in', 'a', 'house', '.']\n",
      "['i', 'go', 'to', 'bed', 'at', '10', 'o', 'clock', 'i', 'watch', 'tv', 'at', '5', 'o', 'clock', 'i', 'live', 'in', 'a', 'house', '.']\n",
      "['the', 'house', 'is', 'in', 'the', 'world', '.']\n",
      "['the', 'house', 'is', 'in', 'the', 'world', '.']\n",
      "['the', 'house', 'is', 'in', 'the', 'world', '.']\n",
      "['i', 'live', 'at', 'boar', 'parva', 'it', 'is', 'near', 'melton', 'and', 'bridgebrook', 'and', 'smallerden', '.']\n",
      "['i', 'live', 'at', 'boar', 'parva', 'it', 'is', 'near', 'melton', 'and', 'bridgebrook', 'and', 'smallerden', '.']\n",
      "['i', 'live', 'at', 'boar', 'parva', 'it', 'is', 'near', 'melton', 'and', 'bridgebrook', 'and', 'smallerden', '.']\n",
      "['the', 'house', 'is', 'white', 'it', 'has', 'stone', 'up', 'the', 'frount', 'it', 'is', 'the', 'first', 'from', 'bridgebrook', 'and', 'the', 'sexeon', 'from', 'smallerden', '.']\n",
      "['the', 'house', 'is', 'white', 'it', 'has', 'stone', 'up', 'the', 'front', 'it', 'is', 'the', 'first', 'from', 'bridgebrook', 'and', 'the', 'sexton', 'from', 'smallerden', '.']\n",
      "['the', 'house', 'is', 'white', 'it', 'has', 'stone', 'up', 'the', 'front', 'it', 'is', 'the', 'first', 'from', 'bridgebrook', 'and', 'the', 'second', 'from', 'smallerden', '.']\n",
      "['my', 'mum', 'is', 'at', 'home', 'she', 'goes', 'to', 'the', 'shop', 'on', 'fridays', 'my', 'dad', 'goes', 'to', 'work', 'at', 'smallerden', '.']\n",
      "['my', 'mum', 'is', 'at', 'home', 'she', 'goes', 'to', 'the', 'shop', 'on', 'fridays', 'my', 'dad', 'goes', 'to', 'work', 'at', 'smallerden', '.']\n",
      "['my', 'mum', 'is', 'at', 'home', 'she', 'goes', 'to', 'the', 'shop', 'on', 'fridays', 'my', 'dad', 'goes', 'to', 'work', 'at', 'smallerden', '.']\n",
      "['what', 'i', 'do', 'when', 'i', 'get', 'home', 'from', 'school', '.']\n",
      "['what', 'i', 'do', 'when', 'i', 'get', 'home', 'from', 'school', '.']\n",
      "['what', 'i', 'do', 'when', 'i', 'get', 'home', 'from', 'school', '.']\n",
      "['on', 'monday', 'i', 'sometimes', 'go', 'down', 'the', 'farm', 'in', 'the', 'night', 'i', 'wach', 'tv', 'there', 'is', 'bbc', 'and', 'i.t.v', '.']\n",
      "['on', 'monday', 'i', 'sometimes', 'go', 'down', 'the', 'farm', 'in', 'the', 'night', 'i', 'watch', 'tv', 'there', 'is', 'bbc', 'and', 'i.t.v', '.']\n",
      "['on', 'monday', 'i', 'sometimes', 'go', 'down', 'the', 'farm', 'in', 'the', 'night', 'i', 'watch', 'tv', 'there', 'is', 'bbc', 'and', 'i.t.v', '.']\n",
      "['i', 'like', 'i.t.v', '.']\n",
      "['i', 'like', 'i.t.v', '.']\n",
      "['i', 'like', 'i.t.v', '.']\n",
      "['we', 'call', 'anglia', 'i.t.v', '.']\n",
      "['we', 'call', 'anglia', 'i.t.v', '.']\n",
      "['we', 'call', 'anglia', 'i.t.v', '.']\n",
      "['we', 'have', 'got', 'anglia', 'like', 'to', 'wach', 'cow_boys', '.']\n",
      "['we', 'have', 'got', 'anglia', 'like', 'to', 'watch', 'cowboy', '.']\n",
      "['we', 'have', 'got', 'anglia', 'like', 'to', 'watch', 'cowboys', '.']\n",
      "['on', 'tuesday', 'i', 'get', 'off', 'the', 'bus', 'and', 'some_times', 'in', 'the', 'night', 'i', 'go', 'to', 'the', 'youth', 'colbe', '.']\n",
      "['on', 'tuesday', 'i', 'get', 'off', 'the', 'bus', 'and', 'sometimes', 'in', 'the', 'night', 'i', 'go', 'to', 'the', 'youth', 'club', '.']\n",
      "['on', 'tuesday', 'i', 'get', 'off', 'the', 'bus', 'and', 'sometimes', 'in', 'the', 'night', 'i', 'go', 'to', 'the', 'youth', 'club', '.']\n",
      "['i', 'like', 'to', 'wach', 't.v', '.']\n",
      "['i', 'like', 'to', 'watch', 't.v', '.']\n",
      "['i', 'like', 'to', 'watch', 't.v', '.']\n",
      "['there', 'is', 'a', 'lot', 'of', 'things', 'on', 't.v', '.']\n",
      "['there', 'is', 'a', 'lot', 'of', 'things', 'on', 't.v', '.']\n",
      "['there', 'is', 'a', 'lot', 'of', 'things', 'on', 't.v', '.']\n",
      "['i', 'wach', 'it', 'each', 'night', '.']\n",
      "['i', 'wash', 'it', 'each', 'night', '.']\n",
      "['i', 'watch', 'it', 'each', 'night', '.']\n",
      "['.']\n",
      "['.']\n",
      "['.']\n",
      "['i', 'thing', 'tv', 'is', 'good', 'but', 'people', 'say', 'it', 'gives', 'us', 'squar', 'iyes', '.']\n",
      "['i', 'think', 't.', 'is', 'good', 'but', 'people', 'say', 'it', 'gives', 'us', 'slad', 'idea', '.']\n",
      "['i', 'think', 'tv', 'is', 'good', 'but', 'people', 'say', 'it', 'gives', 'us', 'square', 'eyes', '.']\n",
      "['the', 'murder', 'car', '.']\n",
      "['the', 'murder', 'car', '.']\n",
      "['the', 'murder', 'car', '.']\n",
      "['1', 'night', 'when', 'it', 'was', 'dark', 'about', '12', 'oclock', 'a', 'man', 'was', 'nock', 'down', 'by', 'a', 'car', '.']\n",
      "['1', 'night', 'when', 'it', 'was', 'dark', 'about', '12', \"o'clock\", 'a', 'man', 'was', 'pork', 'down', 'by', 'a', 'car', '.']\n",
      "['1', 'night', 'when', 'it', 'was', 'dark', 'about', '12', \"o'clock\", 'a', 'man', 'was', 'knocked', 'down', 'by', 'a', 'car', '.']\n",
      "['the', 'body', 'was', 'found', 'a', '7', 'oclock', 'in', 'the', 'morning', '.']\n",
      "['the', 'body', 'was', 'found', 'at', '7', 'block', 'in', 'the', 'morning', '.']\n",
      "['the', 'body', 'was', 'found', 'at', '7', \"o'clock\", 'in', 'the', 'morning', '.']\n",
      "['the', 'name', 'of', 'the', 'man', 'who', 'was', 'kild', 'was', 'jack', 'robbinson', 'he', 'has', 'black', 'hair', 'brown', 'eyes', 'blue', 'jacket', 'and', 'blue', 'jeans', '.']\n",
      "['the', 'name', 'of', 'the', 'man', 'who', 'was', 'killed', 'was', 'jack', 'robbinson', 'he', 'has', 'black', 'hair', 'brown', 'eyes', 'blue', 'jacket', 'and', 'blue', 'jeans', '.']\n",
      "['the', 'name', 'of', 'the', 'man', 'who', 'was', 'killed', 'was', 'jack', 'robbinson', 'he', 'has', 'black', 'hair', 'brown', 'eyes', 'blue', 'jacket', 'and', 'blue', 'jeans', '.']\n",
      "['no', 'one', 'see', 'the', 'car', '.']\n",
      "['no', 'one', 'sue', 'the', 'car', '.']\n",
      "['no', 'one', 'saw', 'the', 'car', '.']\n",
      "['there', 'had', 'bean', 'a', 'lot', 'of', 'people', 'nock', 'down', 'by', 'a', 'car', 'so', 'they', 'cald', 'it', 'the', 'murder', 'car', '.']\n",
      "['there', 'had', 'been', 'a', 'lot', 'of', 'people', 'pork', 'down', 'by', 'a', 'car', 'so', 'they', 'called', 'it', 'the', 'murder', 'car', '.']\n",
      "['there', 'had', 'been', 'a', 'lot', 'of', 'people', 'knocked', 'down', 'by', 'a', 'car', 'so', 'they', 'called', 'it', 'the', 'murder', 'car', '.']\n",
      "['the', 'police', 'cam', 'out', 'to', 'look', 'for', 'the', 'car', '.']\n",
      "['the', 'police', 'car', 'out', 'to', 'look', 'for', 'the', 'car', '.']\n",
      "['the', 'police', 'came', 'out', 'to', 'look', 'for', 'the', 'car', '.']\n",
      "['the', 'other', 'cars', 'were', 'black', 'so', 'one', 'of', 'the', 'other', 'people', 'said', '.']\n",
      "['the', 'other', 'cars', 'were', 'black', 'so', 'one', 'of', 'the', 'other', 'people', 'said', '.']\n",
      "['the', 'other', 'cars', 'were', 'black', 'so', 'one', 'of', 'the', 'other', 'people', 'said', '.']\n",
      "['the', 'murder', 'man', 'has', 'a', 'black', 'beard', 'the', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killd', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'hill', 'the', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "['the', 'murder', 'man', 'has', 'a', 'black', 'beard', 'the', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killed', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'hill', 'the', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "['the', 'murder', 'man', 'has', 'a', 'black', 'beard', 'the', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killed', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'hill', 'there', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "['i', 'kissed', 'a', 'girl', 'one', 'night', 'here', 'iyes', 'were', 'burning', 'blue', 'she', 'said', 'o', 'do', 'you', 'love', 'me', 'of', 'course', 'of', 'course', 'i', 'do', '.']\n",
      "['i', 'kissed', 'a', 'girl', 'one', 'night', 'her', 'lines', 'were', 'burning', 'blue', 'she', 'said', 'o', 'do', 'you', 'love', 'me', 'of', 'course', 'of', 'course', 'i', 'do', '.']\n",
      "['i', 'kissed', 'a', 'girl', 'one', 'night', 'her', 'eyes', 'were', 'burning', 'blue', 'she', 'said', 'o', 'do', 'you', 'love', 'me', 'of', 'course', 'of', 'course', 'i', 'do', '.']\n",
      "['bell', 'ringing', '.']\n",
      "['bell', 'ringing', '.']\n",
      "['bell', 'ringing', '.']\n",
      "['when', 'you', 'start', 'bell', 'ringing', 'you', 'haveto', 'ring', 'a', 'bell', 'be_for', 'you', 'can', 'do', 'any_thing', 'als', '.']\n",
      "['when', 'you', 'start', 'bell', 'ringing', 'you', 'hates', 'ring', 'a', 'bell', 'before', 'you', 'can', 'do', 'anything', 'a.m', '.']\n",
      "['when', 'you', 'start', 'bell', 'ringing', 'you', 'have_to', 'ring', 'a', 'bell', 'before', 'you', 'can', 'do', 'anything', 'else', '.']\n",
      "['it', 'takes', 'a', 'lot', 'to', 'ring', 'a', 'bell', 'on', 'the', 'rope', 'there', 'is', 'a', 'sally', '.']\n",
      "['it', 'takes', 'a', 'lot', 'to', 'ring', 'a', 'bell', 'on', 'the', 'rope', 'there', 'is', 'a', 'sally', '.']\n",
      "['it', 'takes', 'a', 'lot', 'to', 'ring', 'a', 'bell', 'on', 'the', 'rope', 'there', 'is', 'a', 'sally', '.']\n",
      "['on', 'the', 'bell', 'there', 'is', 'a', 'weel', '.']\n",
      "['on', 'the', 'bell', 'there', 'is', 'a', 'wheel', '.']\n",
      "['on', 'the', 'bell', 'there', 'is', 'a', 'wheel', '.']\n",
      "['the', 'weel', 'has', 'the', 'rope', 'on', 'it', 'some', 'sally', 'are', 'green', 'and', 'the', 'other', 'are', 'red', 'white', 'and', 'blue', '.']\n",
      "['the', 'waesel', 'has', 'the', 'rope', 'on', 'it', 'some', 'sally', 'are', 'green', 'and', 'the', 'others', 'are', 'red', 'white', 'and', 'blue', '.']\n",
      "['the', 'wheel', 'has', 'the', 'rope', 'on', 'it', 'some', 'sallies', 'are', 'green', 'and', 'the', 'others', 'are', 'red', 'white', 'and', 'blue', '.']\n",
      "['when', 'you', 'can', 'ring', '.']\n",
      "['when', 'you', 'can', 'ring', '.']\n",
      "['when', 'you', 'can', 'ring', '.']\n",
      "['you', 'rings', 'like', 'this', '.']\n",
      "['you', 'ring', 'like', 'this', '.']\n",
      "['you', 'ring', 'like', 'this', '.']\n",
      "['these', 'are', 'the', 'names', 'of', 'them', 'plain', 'hunt', ',', 'plain', 'bob', ',', 'grandsire', 'doubles', '.']\n",
      "['these', 'are', 'the', 'names', 'of', 'them', 'plain', 'hunt', ',', 'plain', 'bob', ',', 'grandsire', 'doubles', '.']\n",
      "['these', 'are', 'the', 'names', 'of', 'them', 'plain', 'hunt', ',', 'plain', 'bob', ',', 'grandsire', 'doubles', '.']\n",
      "['grandsire', 'triples', 'it', 'takes', 'a', 'long', 'time', 'to', 'ring', 'them', '.']\n",
      "['grandsire', 'triples', 'it', 'takes', 'a', 'long', 'time', 'to', 'ring', 'them', '.']\n",
      "['grandsire', 'triples', 'it', 'takes', 'a', 'long', 'time', 'to', 'ring', 'them', '.']\n",
      "['me', 'and', 'my', 'dad', 'bike', 'to', 'melton', '.']\n",
      "['me', 'and', 'my', 'dad', 'bike', 'to', 'melton', '.']\n",
      "['me', 'and', 'my', 'dad', 'bike', 'to', 'melton', '.']\n",
      "['the', 'sick', 'sow', 'of', 'the', 'army', 'court']\n",
      "['the', 'sick', 'sow', 'of', 'the', 'army', 'court']\n",
      "['the', 'sick', 'sow', 'of', 'the', 'army', 'court']\n",
      "['one', 'day', 'sergent', 's', '.', 'm', '.', 'bullimore', 'told', 'hut', '29', 'to', 'clean', 'cynthia', \"'s\", 'pig', 'sty', 'out', '.']\n",
      "['one', 'day', 'sergent', 's', '.', 'm', '.', 'bullimore', 'told', 'hut', '29', 'to', 'clean', 'pint', \"'s\", 'pig', 'sty', 'out', '.']\n",
      "['one', 'day', 'sergent', 's', '.', 'm', '.', 'bullimore', 'told', 'hut', '29', 'to', 'clean', 'cynthia', \"'s\", 'pig', 'sty', 'out', '.']\n",
      "['when', 'hut', '29', 'got', 'there', 'they', 'had', 'to', 'go', 'and', 'get', 'some', 'gass', 'marsks', 'because', 'the', 'smell', 'was', 'to', 'strong']\n",
      "['when', 'hut', '29', 'got', 'there', 'they', 'had', 'to', 'go', 'and', 'get', 'some', 'pass', 'pass', 'because', 'the', 'smell', 'was', 'to', 'strong']\n",
      "['when', 'hut', '29', 'got', 'there', 'they', 'had', 'to', 'go', 'and', 'get', 'some', 'gas', 'masks', 'because', 'the', 'smell', 'was', 'to', 'strong']\n",
      "['when', 's', 'm', 'bullimore', 'came', 'to', 'the', 'pig', 'sty', 'the', 'pig', 'was', 'laid', 'out', 'on', 'the', 'foor']\n",
      "['when', 's', 'm', 'bullimore', 'came', 'to', 'the', 'pig', 'sty', 'the', 'pig', 'was', 'laid', 'out', 'on', 'the', 'motor']\n",
      "['when', 's', 'm', 'bullimore', 'came', 'to', 'the', 'pig', 'sty', 'the', 'pig', 'was', 'laid', 'out', 'on', 'the', 'floor']\n",
      "['when', 'they', 'came', 'back', 's', '.', 'm', '.', 'bullimore', 'said', 'wat', 'have', 'you', 'been', 'doing', 'to', 'my', 'sow']\n",
      "['when', 'they', 'came', 'back', 's', '.', 'm', '.', 'bullimore', 'said', 'what', 'have', 'you', 'been', 'doing', 'to', 'my', 'sow']\n",
      "['when', 'they', 'came', 'back', 's', '.', 'm', '.', 'bullimore', 'said', 'what', 'have', 'you', 'been', 'doing', 'to', 'my', 'sow']\n",
      "['we', 'have', 'not', 'done', 'anything', 'we', 'had', 'to', 'go', 'and', 'get', 'some', 'gass', 'marsks', 'because', 'the', 'smell', 'was', 'to', 'strong']\n",
      "['we', 'have', 'not', 'done', 'anything', 'we', 'had', 'to', 'go', 'and', 'get', 'some', 'pass', 'pass', 'because', 'the', 'smell', 'was', 'to', 'strong']\n",
      "['we', 'have', 'not', 'done', 'anything', 'we', 'had', 'to', 'go', 'and', 'get', 'some', 'gas', 'masks', 'because', 'the', 'smell', 'was', 'too', 'strong']\n",
      "['that', 'is', 'only', 'the', 'soap', 'wat', 'i', 'wosh', 'it', 'with']\n",
      "['that', 'is', 'only', 'the', 'soap', 'what', 'i', 'won', 'it', 'with']\n",
      "['that', 'is', 'only', 'the', 'soap', 'what', 'i', 'wash', 'it', 'with']\n",
      "['prasp', 'he', 'et', 'some', 'soap']\n",
      "['pass', 'he', '`i', 'some', 'soap']\n",
      "['perhaps', 'he', 'ate', 'some', 'soap']\n",
      "['flogger', 'said', 'shall', 'i', 'go', 'and', 'get', 'the', 'vet']\n",
      "['flogger', 'said', 'shall', 'i', 'go', 'and', 'get', 'the', 'vet']\n",
      "['flogger', 'said', 'shall', 'i', 'go', 'and', 'get', 'the', 'vet']\n",
      "['get', 'back', 'in', 'the', 'line', 'be_for', 'i', 'put', 'you', 'on', 'charge']\n",
      "['get', 'back', 'in', 'the', 'line', 'before', 'i', 'put', 'you', 'on', 'charge']\n",
      "['get', 'back', 'in', 'the', 'line', 'before', 'i', 'put', 'you', 'on', 'charge']\n",
      "['what', 'did', 'you', 'say']\n",
      "['what', 'did', 'you', 'say']\n",
      "['what', 'did', 'you', 'say']\n",
      "['o', 'nothing', 'sir']\n",
      "['o', 'nothing', 'sir']\n",
      "['o', 'nothing', 'sir']\n",
      "['go', 'and', 'get', 'some', 'wiskey']\n",
      "['go', 'and', 'get', 'some', 'wicked']\n",
      "['go', 'and', 'get', 'some', 'whiskey']\n",
      "['okay', 'sir']\n",
      "['okay', 'sir']\n",
      "['okay', 'sir']\n",
      "['put', 'some', 'soap', 'in', 'it', 'flogger', 'said', 'oswald']\n",
      "['put', 'some', 'soap', 'in', 'it', 'flogger', 'said', 'oswald']\n",
      "['put', 'some', 'soap', 'in', 'it', 'flogger', 'said', 'oswald']\n",
      "['ok', 'said', 'flogger']\n",
      "['ok', 'said', 'flogger']\n",
      "['ok', 'said', 'flogger']\n",
      "['flogger', 'went', 'off']\n",
      "['flogger', 'went', 'off']\n",
      "['flogger', 'went', 'off']\n",
      "['a', 'little', 'while', 'after', 'he', 'came', 'with', 'a', 'large', 'soape', 'wiskey']\n",
      "['a', 'little', 'while', 'after', 'he', 'came', 'with', 'a', 'large', 'dare', 'wicked']\n",
      "['a', 'little', 'while', 'after', 'he', 'came', 'with', 'a', 'large', 'soap', 'whiskey']\n",
      "['he', 'cave', 'it', 'to', 's', '.', 'm', '.', 'bullimore']\n",
      "['he', 'gave', 'it', 'to', 's', '.', 'm', '.', 'bullimore']\n",
      "['he', 'gave', 'it', 'to', 's', '.', 'm', '.', 'bullimore']\n",
      "['then', 'he', 'cave', 'it', 'to', 'the', 'pig']\n",
      "['then', 'he', 'gave', 'it', 'to', 'the', 'pig']\n",
      "['then', 'he', 'gave', 'it', 'to', 'the', 'pig']\n",
      "['a', 'little', 'while', 'after', 'the', 'pig', 'go', 'up', 'and', 'there', 'beside', 'here', 'were', 'six', 'little', 'pigs']\n",
      "['a', 'little', 'while', 'after', 'the', 'pig', 'got', 'up', 'and', 'there', 'beside', 'her', 'were', 'six', 'little', 'pigs']\n",
      "['a', 'little', 'while', 'after', 'the', 'pig', 'got', 'up', 'and', 'there', 'beside', 'her', 'were', 'six', 'little', 'pigs']\n",
      "['s', 'm', 'bullimore', 'said', 'that', 'made', 'here', 'better', 'that', 'was', 'not', 'me', 'that', 'was', 'the', 'sope']\n",
      "['s', 'm', 'bullimore', 'said', 'that', 'made', 'her', 'better', 'that', 'was', 'not', 'me', 'that', 'was', 'the', 'done']\n",
      "['s', 'm', 'bullimore', 'said', 'that', 'made', 'her', 'better', 'that', 'was', 'not', 'me', 'that', 'was', 'the', 'soap']\n",
      "['you', 'are', 'on', 'charge']\n",
      "['you', 'are', 'on', 'charge']\n",
      "['you', 'are', 'on', 'charge']\n",
      "['wat', 'did', 'you', 'give', 'here']\n",
      "['heat', 'did', 'you', 'give', 'her']\n",
      "['what', 'did', 'you', 'give', 'her']\n",
      "['i', 'cave', 'her', 'some', 'wiskey', 'and', 'sope', 'you', 'cave', 'her', 'that']\n",
      "['i', 'dare', 'her', 'some', 'whiskey', 'and', 'ripe', 'you', 'dare', 'her', 'that']\n",
      "['i', 'gave', 'her', 'some', 'whiskey', 'and', 'soap', 'you', 'gave', 'her', 'that']\n",
      "['no', 'i', 'knver', 'i', 'sir']\n",
      "['no', 'i', 'never', 'i', 'sir']\n",
      "['no', 'i', 'never', 'i', 'sir']\n",
      "['you', 'did']\n",
      "['you', 'did']\n",
      "['you', 'did']\n",
      "['thats', \"'s\", 'one', 'thing', 'sir', 'it', 'has', 'made', 'her', 'better', '.']\n",
      "['thanks', \"'s\", 'one', 'thing', 'sir', 'it', 'has', 'made', 'her', 'better', '.']\n",
      "['that', \"'s\", 'one', 'thing', 'sir', 'it', 'has', 'made', 'her', 'better', '.']\n",
      "['pigs', 'when', 'young', 'up', 'to', 'being', 'kild', 'for', 'bacon']\n",
      "['pigs', 'when', 'young', 'up', 'to', 'being', 'killed', 'for', 'bacon']\n",
      "['pigs', 'when', 'young', 'up', 'to', 'being', 'killed', 'for', 'bacon']\n",
      "['when', 'they', 'are', 'young', 'you', 'have', 'to', 'wate', '3', 'days', 'then', 'you', 'can', 'injeck', 'them', 'for', 'pneumonia', 'diseases']\n",
      "['when', 'they', 'are', 'young', 'you', 'have', 'to', 'wait', '3', 'days', 'then', 'you', 'can', 'indeed', 'them', 'for', 'pneumonia', 'diseases']\n",
      "['when', 'they', 'are', 'young', 'you', 'have', 'to', 'wait', '3', 'days', 'then', 'you', 'can', 'inject', 'them', 'for', 'pneumonia', 'diseases']\n",
      "['you', 'have', 'mack', 'shore', 'they', 'have', 'drye', 'straw']\n",
      "['you', 'have', 'make', 'sore', 'they', 'have', 'dry', 'straw']\n",
      "['you', 'have', 'make', 'sure', 'they', 'have', 'dry', 'straw']\n",
      "['when', 'you', 'clean', 'them', 'out', 'you', 'should', 'not', 'leave', 'a', 'falk', 'in', 'with', 'them', 'because', 'the', 'mother', 'might', 'nock', 'it', 'down', 'and', 'the', 'little', 'pigs', 'might', 'stab', 'them_souve']\n",
      "['when', 'you', 'clean', 'them', 'out', 'you', 'should', 'not', 'leave', 'a', 'mask', 'in', 'with', 'them', 'because', 'the', 'mother', 'might', 'knock', 'it', 'down', 'and', 'the', 'little', 'pigs', 'might', 'stab', 'themselves']\n",
      "['when', 'you', 'clean', 'them', 'out', 'you', 'should', 'not', 'leave', 'a', 'fork', 'in', 'with', 'them', 'because', 'the', 'mother', 'might', 'knock', 'it', 'down', 'and', 'the', 'little', 'pigs', 'might', 'stab', 'themselves']\n",
      "['we', 'give', 'the', 'worme', 'pouder']\n",
      "['we', 'give', 'them', 'worked', 'cover']\n",
      "['we', 'give', 'them', 'worm', 'powder']\n",
      "['that', 'is', 'when', 'they', 'get', 'the', 'worme']\n",
      "['that', 'is', 'when', 'they', 'get', 'the', 'worked']\n",
      "['that', 'is', 'when', 'they', 'get', 'the', 'worm']\n",
      "['this', 'will', 'stop', 'them', 'from', 'going', 'thin']\n",
      "['this', 'will', 'stop', 'them', 'from', 'going', 'thin']\n",
      "['this', 'will', 'stop', 'them', 'from', 'going', 'thin']\n",
      "['you', 'should', 'box', 'a', 'little', 'place', 'off', 'so', 'only', 'the', 'little', 'pigs', 'can', 'get', 'in', 'it']\n",
      "['you', 'should', 'box', 'a', 'little', 'place', 'off', 'so', 'only', 'the', 'little', 'pigs', 'can', 'get', 'in', 'it']\n",
      "['you', 'should', 'box', 'a', 'little', 'place', 'off', 'so', 'only', 'the', 'little', 'pigs', 'can', 'get', 'in', 'it']\n",
      "['that', 'is', 'so', 'they', 'can', 'ge', 'out', 'of', 'the', 'way', 'of', 'there', 'mother']\n",
      "['that', 'is', 'so', 'they', 'can', 'get', 'out', 'of', 'the', 'way', 'of', 'these', 'mother']\n",
      "['that', 'is', 'so', 'they', 'can', 'get', 'out', 'of', 'the', 'way', 'of', 'their', 'mother']\n",
      "['some', 'people', 'put', 'a', 'light', 'in', 'with', 'theme', 'to', 'geep', 'them', 'warm']\n",
      "['some', 'people', 'put', 'a', 'light', 'in', 'with', 'them', 'to', 'beef', 'them', 'warm']\n",
      "['some', 'people', 'put', 'a', 'light', 'in', 'with', 'them', 'to', 'keep', 'them', 'warm']\n",
      "['you', 'have', 'to', 'make', 'shore', 'that', 'mother', 'has', 'a', 'lot', 'of', 'milk', '.']\n",
      "['you', 'have', 'to', 'make', 'sore', 'that', 'mother', 'has', 'a', 'lot', 'of', 'milk', '.']\n",
      "['you', 'have', 'to', 'make', 'sure', 'that', 'mother', 'has', 'a', 'lot', 'of', 'milk', '.']\n",
      "['if', 'she', 'as', 'not', 'got', 'a_nougth', 'milk', 'you', 'will', 'have', 'to', 'feed', 'them', 'on', 'a', 'bottle']\n",
      "['if', 'she', 'has', 'not', 'got', 'enough', 'milk', 'you', 'will', 'have', 'to', 'feed', 'them', 'on', 'a', 'bottle']\n",
      "['if', 'she', 'has', 'not', 'got', 'enough', 'milk', 'you', 'will', 'have', 'to', 'feed', 'them', 'on', 'a', 'bottle']\n",
      "['when', 'they', 'came', 'eat', 'a', 'little', 'bit', 'you', 'can', 'get', 'them', 'some', 'little', 'nuts', 'of', 'fating']\n",
      "['when', 'they', 'came', 'eat', 'a', 'little', 'bit', 'you', 'can', 'get', 'them', 'some', 'little', 'nuts', 'o_', 'bang']\n",
      "['when', 'they', 'can', 'eat', 'a', 'little', 'bit', 'you', 'can', 'get', 'them', 'some', 'little', 'nuts', 'for', 'fattening']\n",
      "['they', 'can', 'eate', 'some', 'meal', 'when', 'they', 'get', 'a', 'little', 'biger']\n",
      "['they', 'can', 'eat', 'some', 'meal', 'when', 'they', 'get', 'a', 'little', 'tigers']\n",
      "['they', 'can', 'eat', 'some', 'meal', 'when', 'they', 'get', 'a', 'little', 'bigger']\n",
      "['we', 'give', 'them', 'some', 'fating', 'food', 'called', 'nomber', '2']\n",
      "['we', 'give', 'them', 'some', 'fins', 'food', 'called', 'nomber', '2']\n",
      "['we', 'give', 'them', 'some', 'fattening', 'food', 'called', 'nomber', '2']\n",
      "['whe', 'you', 'wean', 'that', 'is', 'take', 'them', 'from', 'their', 'mother', 'you', 'have', 'got', 'to', 'see', 'if', 'they', 'fight', 'if', 'there', 'is', 'any', 'little', 'wones']\n",
      "['wheat', 'you', 'wean', 'that', 'is', 'take', 'them', 'from', 'their', 'mother', 'you', 'have', 'got', 'to', 'see', 'if', 'they', 'fight', 'if', 'there', 'is', 'any', 'little', 'done']\n",
      "['when', 'you', 'wean', 'that', 'is', 'take', 'them', 'from', 'their', 'mother', 'you', 'have', 'got', 'to', 'see', 'if', 'they', 'fight', 'if', 'there', 'is', 'any', 'little', 'ones']\n",
      "['fighting']\n",
      "['fighting']\n",
      "['fighting']\n",
      "['when', 'they', 'and', 'bing', 'you', 'have', 'to', 'see', 'about', 'waying']\n",
      "['when', 'they', 'sand', 'pint', 'you', 'have', 'to', 'see', 'about', 'lying']\n",
      "['when', 'they', 'are', 'big', 'you', 'have', 'to', 'see', 'about', 'weighing']\n",
      "['when', 'they', 'have', 'been', 'waid', 'and', 'reddy', 'to', 'go', 'away', 'to', 'be', 'kild', '.']\n",
      "['when', 'they', 'have', 'been', 'lad', 'and', 'beady', 'to', 'go', 'away', 'to', 'be', 'killed', '.']\n",
      "['when', 'they', 'have', 'been', 'weighed', 'and', 'ready', 'to', 'go', 'away', 'to', 'be', 'killed', '.']\n",
      "['billy', 'bunter', 'was', 'to', 'large', 'so', 'they', 'sent', 'for', 'im', 'in', 'charge']\n",
      "['billy', 'bunter', 'was', 'too', 'large', 'so', 'they', 'sent', 'for', 'him', 'in', 'charge']\n",
      "['billy', 'bunter', 'was', 'too', 'large', 'so', 'they', 'sent', 'for', 'him', 'in', 'charge']\n",
      "['im', 'in', 'charge', 'was', 'to', 'thin', 'so', 'they', 'sent', 'fro', 'rin', 'tin', 'tin']\n",
      "['`i', 'in', 'charge', 'was', 'too', 'thin', 'so', 'they', 'sent', 'for', 'rin', 'tin', 'tin']\n",
      "['him', 'in', 'charge', 'was', 'too', 'thin', 'so', 'they', 'sent', 'for', 'rin', 'tin', 'tin']\n",
      "['rin', 'tin', 'tin', 'heart', 'is', 'poor', 'so', 'they', 'sent', 'for', 'barbra', 'moore']\n",
      "['rin', 'tin', 'tin', 'heart', 'is', 'poor', 'so', 'they', 'sent', 'for', 'barbra', 'moore']\n",
      "['rin', 'tin', 'tin', 'heart', 'is', 'poor', 'so', 'they', 'sent', 'for', 'barbra', 'moore']\n",
      "['barbra', 'moore', 'was', 'having', 'dinner', 'so', 'the', 'sent', 'for', 'yule', 'brinner']\n",
      "['barbra', 'moore', 'was', 'having', 'dinner', 'so', 'they', 'sent', 'for', 'yule', 'brinner']\n",
      "['barbra', 'moore', 'was', 'having', 'dinner', 'so', 'they', 'sent', 'for', 'yule', 'brinner']\n",
      "['yule', 'brinner', 'sang', 'to', 'high', 'then', 'they', 'went', 'to', 'space', 'in', 'the', 'sky']\n",
      "['yule', 'brinner', 'sang', 'too', 'high', 'then', 'they', 'went', 'too', 'space', 'in', 'the', 'sky']\n",
      "['yule', 'brinner', 'sang', 'too', 'high', 'then', 'they', 'went', 'to', 'space', 'in', 'the', 'sky']\n",
      "['my', 'heart', 'is', 'full', 'of', 'sadness', 'my', 'heart', 'is', 'full', 'of', 'joy', 'it', 'might', 'be', 'my', 'wife', 'or', 'it', 'might', 'be', 'helen', 'of', 'toy']\n",
      "['my', 'heart', 'is', 'full', 'of', 'sadness', 'my', 'heart', 'is', 'full', 'of', 'joy', 'it', 'might', 'be', 'my', 'wife', 'or', 'it', 'might', 'be', 'helen', 'of', 'nor']\n",
      "['my', 'heart', 'is', 'full', 'of', 'sadness', 'my', 'heart', 'is', 'full', 'of', 'joy', 'it', 'might', 'be', 'my', 'wife', 'or', 'it', 'might', 'be', 'helen', 'of', 'troy']\n",
      "['one', 'saturday', 'i', 'though', 'i', 'would', 'go', 'to', 'the', 'races', 'at', 'london']\n",
      "['one', 'saturday', 'i', 'thought', 'i', 'would', 'go', 'to', 'the', 'races', 'at', 'london']\n",
      "['one', 'saturday', 'i', 'thought', 'i', 'would', 'go', 'to', 'the', 'races', 'at', 'london']\n",
      "['i', 'went', 'on', 'my', 'royl', 'enfield']\n",
      "['i', 'went', 'on', 'my', 'royal', 'enfield']\n",
      "['i', 'went', 'on', 'my', 'royal', 'enfield']\n",
      "['they', 'can', 'go', 'quite', 'farst']\n",
      "['they', 'can', 'go', 'quite', 'cart']\n",
      "['they', 'can', 'go', 'quite', 'fast']\n",
      "['this', 'was', 'a', 'royl', 'enfield', 'consulatoin', '?', '_']\n",
      "['this', 'was', 'a', 'royal', 'enfield', 'stimulated', '?', '_']\n",
      "['this', 'was', 'a', 'royal', 'enfield', '_', '?', '_']\n"
     ]
    }
   ],
   "source": [
    "# Looking at results of predictions by trigram improved algorithm\n",
    "\n",
    "for i in range(len(data_test_incorrect)):\n",
    "    print(data_test_incorrect[i])\n",
    "    print(predictions[i])\n",
    "    print(data_test_correct[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
